{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 basic sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1691053dca347bdaebb17abb551b94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48eab8a58ff4f2b8a7f47314f882b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267844284.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964ad77a626b46ce87e9040e8ae4ce4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f99da16bb4442c4850c08e895847c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "senti = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'BERT stands for Bidirectional Encoder Representations from Transformers and it is a state-of-the-art machine learning model used for NLP tasks.'\n",
    "sentence2 = 'i hate you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = senti(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9933399558067322}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 1 sentiment analysis result is POSITIVE,  score is 0.9933\n",
      "sentence 2 sentiment analysis score is NEGATIVE, score is 0.9991\n"
     ]
    }
   ],
   "source": [
    "print('sentence 1 sentiment analysis result is {},  score is {:.4f}'.format(senti(sentence1)[0]['label'], senti(sentence1)[0]['score']))\n",
    "print('sentence 2 sentiment analysis score is {}, score is {:.4f}'.format(senti(sentence2)[0]['label'], senti(sentence2)[0]['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 BERT model on IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at /Users/wegzheng/Downloads/BERT/ and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('/Users/wegzheng/Downloads/BERT/',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbEr.txt', 'test', 'imdb.vocab', 'README', 'train']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# We create a training dataset and a validation \n",
    "# dataset from our \"aclImdb/train\" directory with a 80/20 split.\n",
    "train = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='training', seed=2021)\n",
    "valid = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/aclImdb/train', batch_size=30000, validation_split=0.2, \n",
    "    subset='validation', seed=2021)\n",
    "test = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'data/aclImdb/test/', batch_size=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ge_dataframe(dataset):\n",
    "    for i in dataset.take(1):\n",
    "        text = i[0].numpy()\n",
    "        label = i[1].numpy()\n",
    "    df = pd.DataFrame({'content': text, \n",
    "                       'label': label})\n",
    "    df['content'] = df['content'].str.decode(\"utf-8\") # convert b'i like...' to 'i like ...'\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_train = ge_dataframe(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe Don Baker is an alright to good actor in s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I really wanted to like this film, but so much...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I happened upon this flick on a rainy Sunday, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm writing this because I somehow felt being ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  Joe Don Baker is an alright to good actor in s...      0\n",
       "1  I really wanted to like this film, but so much...      0\n",
       "2  I happened upon this flick on a rainy Sunday, ...      0\n",
       "3  I'm writing this because I somehow felt being ...      0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_valid = ge_dataframe(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_test = ge_dataframe(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bert_tokenizer(df):\n",
    "    sentences = df['content']\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)\n",
    "        input_ids.append(bert_inp['input_ids'])\n",
    "        attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "    input_ids=np.asarray(input_ids)\n",
    "    attention_masks=np.array(attention_masks)\n",
    "    labels=np.array(df['label'])\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tr, attenten_masks_tr, labels_tr = gen_bert_tokenizer(df_train.sample(frac=0.1, random_state=2021))\n",
    "input_ids_va, attenten_masks_va, labels_va = gen_bert_tokenizer(df_valid.sample(frac=0.1, random_state=2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.22 s, sys: 155 ms, total: 5.37 s\n",
      "Wall time: 6.06 s\n"
     ]
    }
   ],
   "source": [
    "%time input_ids_ts, attenten_masks_ts, labels_ts = gen_bert_tokenizer(df_test[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 model compile and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "print('\\nBert Model',bert_model.summary())\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
    "\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "32/32 [==============================] - 347s 11s/step - loss: 0.0695 - accuracy: 0.9810 - val_loss: 0.5296 - val_accuracy: 0.8480\n",
      "Epoch 2/4\n",
      "32/32 [==============================] - 345s 11s/step - loss: 0.0289 - accuracy: 0.9935 - val_loss: 0.5900 - val_accuracy: 0.8440\n",
      "Epoch 3/4\n",
      "32/32 [==============================] - 342s 11s/step - loss: 0.0207 - accuracy: 0.9935 - val_loss: 0.6046 - val_accuracy: 0.8460\n"
     ]
    }
   ],
   "source": [
    "history=bert_model.fit([input_ids_tr,attenten_masks_tr],labels_tr,\n",
    "#                        batch_size=params['BATCH_SIZE'], \n",
    "                       batch_size=64, \n",
    "                       epochs=4, \n",
    "                       validation_data=([input_ids_va,attenten_masks_va],labels_va),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 validation on test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 51s, sys: 3min 16s, total: 8min 7s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%time tf_outputs = bert_model([input_ids_ts,attenten_masks_ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.9329982 , 0.0670018 ],\n",
       "       [0.99812835, 0.0018717 ],\n",
       "       [0.00405917, 0.9959408 ],\n",
       "       [0.00184835, 0.9981516 ],\n",
       "       [0.9883062 , 0.01169381]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was a movie that could have been great if...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Surely this deserves to be in the bottom 10 fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I enjoyed it. There you go, I said it again. I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I agree with all aforementioned comments. This...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When I was 13 or so I was lucky enough to find...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>First off, let me say I wasted Halloween movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love documentaries. They are among my favori...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This wonderful 3 part BBC production is one of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Finally! Other people who have actually seen t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is one of the greatest films ever made. B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label\n",
       "0  This was a movie that could have been great if...      0\n",
       "1  Surely this deserves to be in the bottom 10 fi...      0\n",
       "2  I enjoyed it. There you go, I said it again. I...      1\n",
       "3  I agree with all aforementioned comments. This...      1\n",
       "4  When I was 13 or so I was lucky enough to find...      0\n",
       "5  First off, let me say I wasted Halloween movie...      0\n",
       "6  I love documentaries. They are among my favori...      0\n",
       "7  This wonderful 3 part BBC production is one of...      1\n",
       "8  Finally! Other people who have actually seen t...      1\n",
       "9  This is one of the greatest films ever made. B...      1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was a movie that could have been great if there were not so many unnecessary historical inaccuracies and if the actors had been chosen or made up to look a little more like the real persons (not very difficult). Sissi did not go to Mayerling to see her dead son, she also did not die in the street; they carried her on to the boat and then back to the hotel, which was much more dramatic. I am not sure about the wedding night, but I find it exaggerated that a lady-in-waiting would undress the empress and leave her completely naked (and that in the 1850's) or that the emperor would announce very proudly \"yes I finally laid her\" to the assembled court. As far as I know this was done right away on the first night and nobody rewarded her as if she were a streetwalker. The saving grace of the movie is really Stephane Audran, excellent actress and true to character. : \n",
      " Negative\n",
      "Surely this deserves to be in the bottom 10 films of all time, pity it's just a TV movie. Rubbish that only we British can produce! It perhaps has some merit in the so awful it's good scale. Watch out for scene where they start dancing ! : \n",
      " Negative\n",
      "I enjoyed it. There you go, I said it again. I even bought this movie on DVD and enjoyed it a couple of more times. Call me old fashioned but I prefer movies like this to garbage like Die Hard 4 which hold up the box office and get critical acclaim just because you have some old guy saving America. Van Damme moves well for a guy of his age(47 I think), delivering kicks that reminds one of Kickboxer. If you like old school action and and explosions, this is the movie to watch. This is one of Van Damme's best works.<br /><br />Van Damme and Steven Seagal movies get released theatrically where I live so I never miss a chance to watch our old school action stars on the big screen. : \n",
      " Positive\n",
      "I agree with all aforementioned comments. This show was a delight to watch. Funny, witty, terrific acting and zany sets. It's always a thrill to find a show that is smartly written, assumes the audience has brains and displays subtle humor. I would spend good, hard-earned cash money to see it again on DVD. And as long as we're requesting Smart Series That Never Got a Chance...How about DVD releases of Maximum Bob (another well written, odd duck show with a delightful cast of characters.) And add to the list...Middle Ages or Frank's Place. There has to a way to release these shows out of the vaults and into the hands of devoted fans and new audiences. : \n",
      " Positive\n",
      "When I was 13 or so I was lucky enough to find this film. It was part of an endless Danish series of really cheesy stuff. This however was the cheesiest I ever owned - but I guess I sold it, too bad. Well what to write... Better than \"Manos: the hands of fate\" and worse than \"Critters 4\". But it's definitely worth an hour and a half since this was made by people who wanted to make it. The acting isn't that terrible compared to several other eighties trash - in fact I kind of like the old man even though he did'NWT look that Indian to me. But I guess you can't have everything... Do yourselves a favour and look this up... : \n",
      " Negative\n"
     ]
    }
   ],
   "source": [
    "labels = ['Negative', 'Positive']\n",
    "label = tf.argmax(tf_predictions, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(5):\n",
    "    print(df_test['content'][i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 validation on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = ['This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good',\n",
    "                  'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good : \n",
      " Positive\n",
      "One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie : \n",
      " Negative\n"
     ]
    }
   ],
   "source": [
    "tf_batch2 = bert_tokenizer(pred_sentences, max_length=64, padding=True, truncation=True, return_tensors='tf')\n",
    "tf_outputs2 = bert_model(tf_batch2)\n",
    "tf_predictions2 = tf.nn.softmax(tf_outputs2[0], axis=-1)\n",
    "labels = ['Negative', 'Positive']\n",
    "label = tf.argmax(tf_predictions2, axis=1)\n",
    "label = label.numpy()\n",
    "for i in range(len(pred_sentences)):\n",
    "    print(pred_sentences[i], \": \\n\", labels[label[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.00161889, 0.99838114],\n",
       "       [0.9979481 , 0.00205192]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
