{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_ids(tokenizer, texts, labels, output_length=128):\n",
    "    bert_ids = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\", max_length=output_length)\n",
    "    input_ids = np.asarray(bert_ids['input_ids'])\n",
    "    attention_mask = np.asarray(bert_ids['attention_mask'])\n",
    "    return input_ids, attention_mask, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ChineseNlpCorpus/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>我第一次入住如家，实际情况与现实相差太远，屋子里味道很大，打开窗户换气又很吵，卫生不敢恭维！...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>给朋友定了该酒店,价格比以前高出了很多.这且不说.转天朋友结帐,前台因为染了一块毛巾和床单的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>环境不错,地点也很好!下次还会入住!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      0  我第一次入住如家，实际情况与现实相差太远，屋子里味道很大，打开窗户换气又很吵，卫生不敢恭维！...\n",
       "1      0  给朋友定了该酒店,价格比以前高出了很多.这且不说.转天朋友结帐,前台因为染了一块毛巾和床单的...\n",
       "2      1                                 环境不错,地点也很好!下次还会入住!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = df.sample(frac=1, random_state=2021).reset_index(drop=True)\n",
    "df_output.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7766, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train:dev:test as 8:1:1\n",
    "train_df = df_output.iloc[:5000]\n",
    "valid_df = df_output.iloc[5000:6500]\n",
    "test_df = df_output.iloc[6500:]\n",
    "\n",
    "train_df.to_csv('data/ChineseNlpCorpus/datasets/ChnSentiCorp_htl_all/cleaned/train.csv', sep=',', index=False)\n",
    "valid_df.to_csv('data/ChineseNlpCorpus/datasets/ChnSentiCorp_htl_all/cleaned/valid.csv', sep=',', index=False)\n",
    "test_df.to_csv('data/ChineseNlpCorpus/datasets/ChnSentiCorp_htl_all/cleaned/test.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法1 jieba分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from https://github.com/stopwords-iso/stopwords-zh\n",
    "stopwords = [ w.strip() for w in codecs.open('data/stopwords-zh/stopwords-zh.txt', 'r', encoding='utf-8') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "def clearTxt(line):\n",
    "    if line != '':\n",
    "        line = line.strip()\n",
    "        #remove english and numbers\n",
    "        line = re.sub(\"[a-zA-Z0-9]\",\"\",line)\n",
    "         \n",
    "        #去除文本中的中文符号和英文符号\n",
    "        line = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'；：“”．]+|[+——！，。？?、~@#￥%……&*（）]+\", \"\", line)\n",
    "        return line\n",
    "    else:\n",
    "        return 'Empyt Line'\n",
    " \n",
    "# text cut, remove stopwords\n",
    "def sent2word(line):\n",
    "     \n",
    "    segList = jieba.cut(line, cut_all=False)\n",
    "    segSentence = ''\n",
    "    for word in segList:\n",
    "        if word != '\\t' and ( word not in stopwords ):\n",
    "            segSentence += ( word + \" \" )\n",
    "    return segSentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/rb/2_frt6zn7qz8ybmdwqwmjd100000gn/T/jieba.cache\n",
      "Loading model cost 0.723 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "test = jieba.cut(clearTxt(df['review'][0]))\n",
    "sentence = ''\n",
    "for w in test:\n",
    "    if w != '\\t' and (w not in stopwords):\n",
    "        sentence += (w + \" \")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较为简单.\n",
      " \n",
      "距离 川沙 公路 较近 公交 指示 蔡陆线 非常 麻烦 建议 路线 房间 较为简单 \n"
     ]
    }
   ],
   "source": [
    "print(df['review'][0])\n",
    "print(' ')\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\", model_max_length=5000)\n",
    "# bert_tokenizer.save_pretrained(\"/Users/wegzheng/Downloads/bert-base-chinses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at /Users/wegzheng/Downloads/bert-base-chinses and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained('/Users/wegzheng/Downloads/bert-base-chinses',num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>我第一次入住如家，实际情况与现实相差太远，屋子里味道很大，打开窗户换气又很吵，卫生不敢恭维！...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>给朋友定了该酒店,价格比以前高出了很多.这且不说.转天朋友结帐,前台因为染了一块毛巾和床单的...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>环境不错,地点也很好!下次还会入住!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      0  我第一次入住如家，实际情况与现实相差太远，屋子里味道很大，打开窗户换气又很吵，卫生不敢恭维！...\n",
       "1      0  给朋友定了该酒店,价格比以前高出了很多.这且不说.转天朋友结帐,前台因为染了一块毛巾和床单的...\n",
       "2      1                                 环境不错,地点也很好!下次还会入住!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我第一次入住如家，实际情况与现实相差太远，屋子里味道很大，打开窗户换气又很吵，卫生不敢恭维！入住时间不长就搬走。而且位置也很偏，不知道为何在这里选址！总之不会再住！楼下涮肉一般！',\n",
       " '给朋友定了该酒店,价格比以前高出了很多.这且不说.转天朋友结帐,前台因为染了一块毛巾和床单的说法要求赔偿110元.朋友很不开心,说就是喝你们冰箱可乐时候喷出来一些.而且110买你5套也够了.结果我要求前台把经理找来,一个小伙子说他就是.朋友说你们四星这么做有点过火.后来我要求那个小伙子带我去房间看一下有没有污染,到什么程度.于是他就一起去了,床单的一大片我表示怀疑是否是朋友染的.于是在我拉掉床单的一刹那,令人惊讶的事情发生了,在下面棉单上面,竟然有一女人例假的血迹.当然就想吐,那血还好象是近期的.朋友一看就急了,在这床上睡一晚是多么让人恶心.我相信酒店也完全能检查出来有,根本没有换,或者简单洗完就铺上了.所以,请大家今后住这个酒店一定好好检查检查.(酒店最终解决是半价,但是远远无法消除这种失望,决心不再入住).补充点评2007年2月3日：如果床单上发现有血迹,这在国外,甚至国内的4,5星酒店绝对是无法容忍的事情.酒店不是简单的打着折就能摆平的事情.',\n",
       " '环境不错,地点也很好!下次还会入住!',\n",
       " '大堂门口外管理停车的保安不尊重客人，一辆市府领导的车停在对面，那一长条的车位就不准停车。更重要的事出言不逊，“这地方是你社会车辆能停的吗？”真让我在美国同事（懂汉语）面前觉得丢中国人的脸！',\n",
       " '1.希望房间的改造加快进度,改造完房间不错,没有改造好的房间的确是三星的感觉.2.早餐很有特色,很不错.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in train_df['review'][:5].astype('str')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.46 s, sys: 26.3 ms, total: 6.48 s\n",
      "Wall time: 6.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "input_ids_tr, att_mask_tr, label_tr = generate_bert_ids(bert_tokenizer, train_df['review'].astype('str').to_list(), train_df['label'].values)\n",
    "input_ids_va, att_mask_va, label_va = generate_bert_ids(bert_tokenizer, valid_df['review'].astype('str').to_list(), valid_df['label'].values)\n",
    "input_ids_ts, att_mask_ts, label_ts = generate_bert_ids(bert_tokenizer, test_df['review'].astype('str').to_list(), test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of data: 5000\n",
      "lenth of data: 1500\n",
      "lenth of data: 1266\n"
     ]
    }
   ],
   "source": [
    "print(\"lenth of data: {}\".format(len(input_ids_tr)))\n",
    "print(\"lenth of data: {}\".format(len(input_ids_va)))\n",
    "print(\"lenth of data: {}\".format(len(input_ids_ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 2769, 5018,  671, 3613, 1057,  857, 1963, 2157, 8024, 2141,\n",
       "       7354, 2658, 1105,  680, 4385, 2141, 4685, 2345, 1922, 6823, 8024,\n",
       "       2238, 2094, 7027, 1456, 6887, 2523, 1920, 8024, 2802, 2458, 4970,\n",
       "       2787, 2940, 3698, 1348, 2523, 1427, 8024, 1310, 4495,  679, 3140,\n",
       "       2621, 5335, 8013, 1057,  857, 3198, 7313,  679, 7270, 2218, 3021,\n",
       "       6624,  511, 5445,  684,  855, 5390,  738, 2523,  974, 8024,  679,\n",
       "       4761, 6887,  711,  862, 1762, 6821, 7027, 6848, 1770, 8013, 2600,\n",
       "        722,  679,  833, 1086,  857, 8013, 3517,  678, 3888, 5489,  671,\n",
       "       5663, 8013,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_tr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ids length is 128\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ids length is {}\".format(len(input_ids_tr[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-04)\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, mode='auto', restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/logging/__init__.py:8: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  # not be used in advertising or publicity pertaining to distribution\n",
      "2021-03-20 12:10:53,545 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:10:53,626 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:10:57,225 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:10:57,250 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9752 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:46:46,336 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 12:46:46,389 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2487s 16s/step - loss: 0.0752 - accuracy: 0.9752 - val_loss: 0.2991 - val_accuracy: 0.8953\n",
      "Epoch 2/3\n",
      "157/157 [==============================] - 6769s 43s/step - loss: 0.0570 - accuracy: 0.9820 - val_loss: 0.3200 - val_accuracy: 0.8987\n",
      "Epoch 3/3\n",
      "157/157 [==============================] - 2455s 16s/step - loss: 0.0341 - accuracy: 0.9912 - val_loss: 0.3833 - val_accuracy: 0.8993\n",
      "CPU times: user 12h 32min 2s, sys: 3h 54min 11s, total: 16h 26min 13s\n",
      "Wall time: 3h 15min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = bert_model.fit([input_ids_tr,att_mask_tr],label_tr,\n",
    "                       batch_size=32, \n",
    "                       epochs=3, \n",
    "                       validation_data=([input_ids_va,att_mask_va],label_va),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 15:26:04,969 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 15:26:05,022 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 25s, sys: 5min 43s, total: 25min 9s\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%time preds = bert_model.predict([input_ids_ts,att_mask_ts],batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9115323854660348"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arr = tf.nn.softmax(preds[0], axis=-1)\n",
    "pred_labels = tf.argmax(pred_arr, axis=1).numpy()\n",
    "accuracy_score(label_ts, pred_labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 raw keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from keras.preprocessing import sequence, text\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using keras tokenizer here\n",
    "# token = text.Tokenizer(num_words=20000) # or use None)\n",
    "# max_len = 1500\n",
    "\n",
    "# train_texts = train_df['review'].astype('str').to_list()\n",
    "# val_texts = valid_df['review'].astype('str').to_list()\n",
    "# test_texts = test_df['review'].astype('str').to_list()\n",
    "\n",
    "# token.fit_on_texts(list(train_texts) + list(val_texts) + list(test_texts))\n",
    "# xtrain_seq = token.texts_to_sequences(train_texts)\n",
    "# xvalid_seq = token.texts_to_sequences(val_texts)\n",
    "# xtest_seq = token.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token index length is 25316\n"
     ]
    }
   ],
   "source": [
    "word_index = token.word_index\n",
    "print(\"token index length is {}\".format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(input_ids_tr, maxlen=128)\n",
    "xvalid_pad = sequence.pad_sequences(input_ids_va, maxlen=128)\n",
    "xtest_pad = sequence.pad_sequences(input_ids_ts, maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1266, 128)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 1920, 1828, 7305, 1366, 1912, 5052, 4415,  977, 6756, 4638,\n",
       "        924, 2128,  679, 2203, 7028, 2145,  782, 8024,  671, 6775, 2356,\n",
       "       2424, 7566, 2193, 4638, 6756,  977, 1762, 2190, 7481, 8024, 6929,\n",
       "        671, 7270, 3340, 4638, 6756,  855, 2218,  679, 1114,  977, 6756,\n",
       "        511, 3291, 7028, 6206, 4638,  752, 1139, 6241,  679, 6849, 8024,\n",
       "        100, 6821, 1765, 3175, 3221,  872, 4852,  833, 6756, 6775, 5543,\n",
       "        977, 4638, 1408, 8043,  100, 4696, 6375, 2769, 1762, 5401, 1744,\n",
       "       1398,  752, 8020, 2743, 3727, 6427, 8021, 7481, 1184, 6230, 2533,\n",
       "        696,  704, 1744,  782, 4638, 5567, 8013,  102,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_pad[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 32)          810144    \n",
      "_________________________________________________________________\n",
      "dropout_62 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_12  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 810,177\n",
      "Trainable params: 810,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create model structure\n",
    "model_keras = tf.keras.Sequential([\n",
    "    layers.Embedding(len(word_index) + 1, 32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "             metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "79/79 [==============================] - 1s 7ms/step - loss: 0.6825 - binary_accuracy: 0.3084 - val_loss: 0.6625 - val_binary_accuracy: 0.3227\n",
      "Epoch 2/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.6550 - binary_accuracy: 0.3131 - val_loss: 0.6391 - val_binary_accuracy: 0.3227\n",
      "Epoch 3/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.6321 - binary_accuracy: 0.3524 - val_loss: 0.6209 - val_binary_accuracy: 0.5740\n",
      "Epoch 4/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.6169 - binary_accuracy: 0.5979 - val_loss: 0.6069 - val_binary_accuracy: 0.6893\n",
      "Epoch 5/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5946 - binary_accuracy: 0.7086 - val_loss: 0.5976 - val_binary_accuracy: 0.7553\n",
      "Epoch 6/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5913 - binary_accuracy: 0.7609 - val_loss: 0.5902 - val_binary_accuracy: 0.7633\n",
      "Epoch 7/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5784 - binary_accuracy: 0.7697 - val_loss: 0.5841 - val_binary_accuracy: 0.7647\n",
      "Epoch 8/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5810 - binary_accuracy: 0.7732 - val_loss: 0.5781 - val_binary_accuracy: 0.7640\n",
      "Epoch 9/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5692 - binary_accuracy: 0.7840 - val_loss: 0.5722 - val_binary_accuracy: 0.7680\n",
      "Epoch 10/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5595 - binary_accuracy: 0.7941 - val_loss: 0.5661 - val_binary_accuracy: 0.7693\n",
      "Epoch 11/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5636 - binary_accuracy: 0.7802 - val_loss: 0.5597 - val_binary_accuracy: 0.7727\n",
      "Epoch 12/200\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5616 - binary_accuracy: 0.7732 - val_loss: 0.5531 - val_binary_accuracy: 0.7753\n",
      "Epoch 13/200\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.5406 - binary_accuracy: 0.7907 - val_loss: 0.5463 - val_binary_accuracy: 0.7767\n",
      "Epoch 14/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5314 - binary_accuracy: 0.7951 - val_loss: 0.5392 - val_binary_accuracy: 0.7793\n",
      "Epoch 15/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5207 - binary_accuracy: 0.8029 - val_loss: 0.5321 - val_binary_accuracy: 0.7793\n",
      "Epoch 16/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5118 - binary_accuracy: 0.8061 - val_loss: 0.5248 - val_binary_accuracy: 0.7820\n",
      "Epoch 17/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.5136 - binary_accuracy: 0.7989 - val_loss: 0.5175 - val_binary_accuracy: 0.7880\n",
      "Epoch 18/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4987 - binary_accuracy: 0.8122 - val_loss: 0.5102 - val_binary_accuracy: 0.7873\n",
      "Epoch 19/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4926 - binary_accuracy: 0.8146 - val_loss: 0.5029 - val_binary_accuracy: 0.7880\n",
      "Epoch 20/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4834 - binary_accuracy: 0.8167 - val_loss: 0.4957 - val_binary_accuracy: 0.7927\n",
      "Epoch 21/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4756 - binary_accuracy: 0.8246 - val_loss: 0.4887 - val_binary_accuracy: 0.7913\n",
      "Epoch 22/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4670 - binary_accuracy: 0.8272 - val_loss: 0.4818 - val_binary_accuracy: 0.7987\n",
      "Epoch 23/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4610 - binary_accuracy: 0.8288 - val_loss: 0.4753 - val_binary_accuracy: 0.8007\n",
      "Epoch 24/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4492 - binary_accuracy: 0.8367 - val_loss: 0.4689 - val_binary_accuracy: 0.8047\n",
      "Epoch 25/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4458 - binary_accuracy: 0.8361 - val_loss: 0.4628 - val_binary_accuracy: 0.8087\n",
      "Epoch 26/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4416 - binary_accuracy: 0.8362 - val_loss: 0.4570 - val_binary_accuracy: 0.8080\n",
      "Epoch 27/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4365 - binary_accuracy: 0.8365 - val_loss: 0.4515 - val_binary_accuracy: 0.8113\n",
      "Epoch 28/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4133 - binary_accuracy: 0.8477 - val_loss: 0.4462 - val_binary_accuracy: 0.8160\n",
      "Epoch 29/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4237 - binary_accuracy: 0.8397 - val_loss: 0.4410 - val_binary_accuracy: 0.8173\n",
      "Epoch 30/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4016 - binary_accuracy: 0.8566 - val_loss: 0.4365 - val_binary_accuracy: 0.8193\n",
      "Epoch 31/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4023 - binary_accuracy: 0.8460 - val_loss: 0.4319 - val_binary_accuracy: 0.8193\n",
      "Epoch 32/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3959 - binary_accuracy: 0.8537 - val_loss: 0.4276 - val_binary_accuracy: 0.8247\n",
      "Epoch 33/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3915 - binary_accuracy: 0.8517 - val_loss: 0.4234 - val_binary_accuracy: 0.8253\n",
      "Epoch 34/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3792 - binary_accuracy: 0.8582 - val_loss: 0.4196 - val_binary_accuracy: 0.8293\n",
      "Epoch 35/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3892 - binary_accuracy: 0.8536 - val_loss: 0.4158 - val_binary_accuracy: 0.8300\n",
      "Epoch 36/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3733 - binary_accuracy: 0.8607 - val_loss: 0.4124 - val_binary_accuracy: 0.8300\n",
      "Epoch 37/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3703 - binary_accuracy: 0.8571 - val_loss: 0.4090 - val_binary_accuracy: 0.8293\n",
      "Epoch 38/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3659 - binary_accuracy: 0.8589 - val_loss: 0.4058 - val_binary_accuracy: 0.8307\n",
      "Epoch 39/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3625 - binary_accuracy: 0.8558 - val_loss: 0.4030 - val_binary_accuracy: 0.8313\n",
      "Epoch 40/200\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.3666 - binary_accuracy: 0.8501 - val_loss: 0.4001 - val_binary_accuracy: 0.8327\n",
      "Epoch 41/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3586 - binary_accuracy: 0.8551 - val_loss: 0.3975 - val_binary_accuracy: 0.8320\n",
      "Epoch 42/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3555 - binary_accuracy: 0.8601 - val_loss: 0.3949 - val_binary_accuracy: 0.8327\n",
      "Epoch 43/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3448 - binary_accuracy: 0.8658 - val_loss: 0.3923 - val_binary_accuracy: 0.8320\n",
      "Epoch 44/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3418 - binary_accuracy: 0.8583 - val_loss: 0.3900 - val_binary_accuracy: 0.8347\n",
      "Epoch 45/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3405 - binary_accuracy: 0.8692 - val_loss: 0.3874 - val_binary_accuracy: 0.8353\n",
      "Epoch 46/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3292 - binary_accuracy: 0.8693 - val_loss: 0.3854 - val_binary_accuracy: 0.8340\n",
      "Epoch 47/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3324 - binary_accuracy: 0.8734 - val_loss: 0.3833 - val_binary_accuracy: 0.8373\n",
      "Epoch 48/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3264 - binary_accuracy: 0.8775 - val_loss: 0.3814 - val_binary_accuracy: 0.8393\n",
      "Epoch 49/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3274 - binary_accuracy: 0.8733 - val_loss: 0.3795 - val_binary_accuracy: 0.8380\n",
      "Epoch 50/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3259 - binary_accuracy: 0.8691 - val_loss: 0.3777 - val_binary_accuracy: 0.8367\n",
      "Epoch 51/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3200 - binary_accuracy: 0.8784 - val_loss: 0.3758 - val_binary_accuracy: 0.8353\n",
      "Epoch 52/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3172 - binary_accuracy: 0.8728 - val_loss: 0.3741 - val_binary_accuracy: 0.8360\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3211 - binary_accuracy: 0.8726 - val_loss: 0.3724 - val_binary_accuracy: 0.8373\n",
      "Epoch 54/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3129 - binary_accuracy: 0.8782 - val_loss: 0.3708 - val_binary_accuracy: 0.8367\n",
      "Epoch 55/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3070 - binary_accuracy: 0.8737 - val_loss: 0.3694 - val_binary_accuracy: 0.8360\n",
      "Epoch 56/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3053 - binary_accuracy: 0.8846 - val_loss: 0.3677 - val_binary_accuracy: 0.8367\n",
      "Epoch 57/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2964 - binary_accuracy: 0.8885 - val_loss: 0.3666 - val_binary_accuracy: 0.8360\n",
      "Epoch 58/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2997 - binary_accuracy: 0.8784 - val_loss: 0.3652 - val_binary_accuracy: 0.8353\n",
      "Epoch 59/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2891 - binary_accuracy: 0.8877 - val_loss: 0.3639 - val_binary_accuracy: 0.8347\n",
      "Epoch 60/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2945 - binary_accuracy: 0.8862 - val_loss: 0.3624 - val_binary_accuracy: 0.8360\n",
      "Epoch 61/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2893 - binary_accuracy: 0.8843 - val_loss: 0.3613 - val_binary_accuracy: 0.8360\n",
      "Epoch 62/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2894 - binary_accuracy: 0.8915 - val_loss: 0.3603 - val_binary_accuracy: 0.8360\n",
      "Epoch 63/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3040 - binary_accuracy: 0.8747 - val_loss: 0.3591 - val_binary_accuracy: 0.8380\n",
      "Epoch 64/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2814 - binary_accuracy: 0.8989 - val_loss: 0.3580 - val_binary_accuracy: 0.8373\n",
      "Epoch 65/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2872 - binary_accuracy: 0.8839 - val_loss: 0.3570 - val_binary_accuracy: 0.8373\n",
      "Epoch 66/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2724 - binary_accuracy: 0.8950 - val_loss: 0.3560 - val_binary_accuracy: 0.8380\n",
      "Epoch 67/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2767 - binary_accuracy: 0.8933 - val_loss: 0.3550 - val_binary_accuracy: 0.8373\n",
      "Epoch 68/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2816 - binary_accuracy: 0.8924 - val_loss: 0.3542 - val_binary_accuracy: 0.8360\n",
      "Epoch 69/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2841 - binary_accuracy: 0.8877 - val_loss: 0.3532 - val_binary_accuracy: 0.8347\n",
      "Epoch 70/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2735 - binary_accuracy: 0.8887 - val_loss: 0.3526 - val_binary_accuracy: 0.8360\n",
      "Epoch 71/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2713 - binary_accuracy: 0.9000 - val_loss: 0.3518 - val_binary_accuracy: 0.8367\n",
      "Epoch 72/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2760 - binary_accuracy: 0.8902 - val_loss: 0.3512 - val_binary_accuracy: 0.8373\n",
      "Epoch 73/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2716 - binary_accuracy: 0.8936 - val_loss: 0.3502 - val_binary_accuracy: 0.8373\n",
      "Epoch 74/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2662 - binary_accuracy: 0.8925 - val_loss: 0.3496 - val_binary_accuracy: 0.8393\n",
      "Epoch 75/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2581 - binary_accuracy: 0.8991 - val_loss: 0.3492 - val_binary_accuracy: 0.8373\n",
      "Epoch 76/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2641 - binary_accuracy: 0.8923 - val_loss: 0.3481 - val_binary_accuracy: 0.8433\n",
      "Epoch 77/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2649 - binary_accuracy: 0.8986 - val_loss: 0.3480 - val_binary_accuracy: 0.8373\n",
      "Epoch 78/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2543 - binary_accuracy: 0.9029 - val_loss: 0.3475 - val_binary_accuracy: 0.8380\n",
      "Epoch 79/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2540 - binary_accuracy: 0.9036 - val_loss: 0.3470 - val_binary_accuracy: 0.8387\n",
      "Epoch 80/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2545 - binary_accuracy: 0.9004 - val_loss: 0.3460 - val_binary_accuracy: 0.8453\n",
      "Epoch 81/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2572 - binary_accuracy: 0.8985 - val_loss: 0.3458 - val_binary_accuracy: 0.8420\n",
      "Epoch 82/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2563 - binary_accuracy: 0.9009 - val_loss: 0.3454 - val_binary_accuracy: 0.8393\n",
      "Epoch 83/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2471 - binary_accuracy: 0.9029 - val_loss: 0.3451 - val_binary_accuracy: 0.8427\n",
      "Epoch 84/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2411 - binary_accuracy: 0.9077 - val_loss: 0.3448 - val_binary_accuracy: 0.8413\n",
      "Epoch 85/200\n",
      "79/79 [==============================] - 0s 6ms/step - loss: 0.2533 - binary_accuracy: 0.9078 - val_loss: 0.3442 - val_binary_accuracy: 0.8440\n",
      "Epoch 86/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2459 - binary_accuracy: 0.9081 - val_loss: 0.3440 - val_binary_accuracy: 0.8453\n",
      "Epoch 87/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2369 - binary_accuracy: 0.9081 - val_loss: 0.3437 - val_binary_accuracy: 0.8440\n",
      "Epoch 88/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2442 - binary_accuracy: 0.9059 - val_loss: 0.3432 - val_binary_accuracy: 0.8467\n",
      "Epoch 89/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2553 - binary_accuracy: 0.8961 - val_loss: 0.3430 - val_binary_accuracy: 0.8473\n",
      "Epoch 90/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2433 - binary_accuracy: 0.9048 - val_loss: 0.3427 - val_binary_accuracy: 0.8487\n",
      "Epoch 91/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2497 - binary_accuracy: 0.9064 - val_loss: 0.3427 - val_binary_accuracy: 0.8493\n",
      "Epoch 92/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2324 - binary_accuracy: 0.9085 - val_loss: 0.3426 - val_binary_accuracy: 0.8480\n",
      "Epoch 93/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2297 - binary_accuracy: 0.9090 - val_loss: 0.3426 - val_binary_accuracy: 0.8460\n",
      "Epoch 94/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2291 - binary_accuracy: 0.9084 - val_loss: 0.3424 - val_binary_accuracy: 0.8487\n",
      "Epoch 95/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2300 - binary_accuracy: 0.9047 - val_loss: 0.3426 - val_binary_accuracy: 0.8453\n",
      "Epoch 96/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2341 - binary_accuracy: 0.9114 - val_loss: 0.3424 - val_binary_accuracy: 0.8460\n",
      "CPU times: user 2min 33s, sys: 18.5 s, total: 2min 51s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the model\n",
    "epochs = 200\n",
    "history = model_keras.fit(xtrain_pad, np.array(label_tr),\n",
    "                       batch_size=64, \n",
    "                       epochs=epochs, \n",
    "                       validation_data=(xvalid_pad,np.array(label_va)),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854660347551343"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_keras.predict_classes(xtest_pad)\n",
    "accuracy_score(label_ts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 baidu ERNIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 baidu raw tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install paddlepaddle -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ERNIE'...\n",
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 2034 (delta 1), reused 3 (delta 1), pack-reused 2027\u001b[K\n",
      "Receiving objects: 100% (2034/2034), 88.86 MiB | 1.70 MiB/s, done.\n",
      "Resolving deltas: 100% (1106/1106), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/PaddlePaddle/ERNIE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/site-packages (from -r ERNIE/requirements.txt (line 1)) (1.19.5)\n",
      "Collecting pyzmq==18.0.2\n",
      "  Downloading pyzmq-18.0.2-cp37-cp37m-macosx_10_9_x86_64.whl (812 kB)\n",
      "\u001b[K     |████████████████████████████████| 812 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.11.0\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sklearn==0.0 in /Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/site-packages (from -r ERNIE/requirements.txt (line 4)) (0.0)\n",
      "Collecting sentencepiece==0.1.8\n",
      "  Downloading sentencepiece-0.1.8-cp37-cp37m-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 107 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jieba==0.39\n",
      "  Downloading jieba-0.39.zip (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting visualdl>=2.0.0b7\n",
      "  Downloading visualdl-2.1.1-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 111 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathlib2>=2.3.2\n",
      "  Downloading pathlib2-2.3.5-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tqdm>=4.32.2 in /Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/site-packages (from -r ERNIE/requirements.txt (line 9)) (4.54.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/site-packages (from sklearn==0.0->-r ERNIE/requirements.txt (line 4)) (0.21.3)\n",
      "Collecting bce-python-sdk\n",
      "  Downloading bce_python_sdk-0.8.59-py3-none-any.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.11.0 in /Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/site-packages (from visualdl>=2.0.0b7->-r ERNIE/requirements.txt (line 7)) (3.14.0)\n",
      "Collecting Flask-Babel>=1.0.0\n",
      "  Downloading Flask_Babel-2.0.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting visualdl>=2.0.0b7\n",
      "  Downloading visualdl-2.1.0-py3-none-any.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 102 kB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading visualdl-2.0.5-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading visualdl-2.0.4-py3-none-any.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading visualdl-2.0.3-py3-none-any.whl (5.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.0 MB 101 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading visualdl-2.0.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 102 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading visualdl-2.0.1-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 116 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/site-packages (from visualdl>=2.0.0b7->-r ERNIE/requirements.txt (line 7)) (4.5.1.48)\n",
      "  Downloading visualdl-2.0.0-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading visualdl-2.0.0b8-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading visualdl-2.0.0b7-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 122 kB/s eta 0:00:01\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tqdm to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tqdm>=4.32.2\n",
      "  Downloading tqdm-4.59.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.58.0-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.57.0-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.56.2-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.56.1-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.56.0-py2.py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.55.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.55.1-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.55.0-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.54.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.53.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 5.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.52.0-py2.py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 4.6 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 5.1 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.50.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 92 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.50.1-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.50.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 6.0 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 5.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.48.2-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.48.1-py2.py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 6.0 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.48.0-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 5.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.47.0-py2.py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 5.2 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.46.1-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.3 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.46.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 5.1 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.44.1-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 5.3 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.44.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 5.0 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.43.0-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.42.0-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 154 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.41.1-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.40.2-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.40.1-py2.py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 4.1 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.40.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.8 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.39.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.38.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 3.1 MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Downloading tqdm-4.37.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 3.1 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.36.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.36.0-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 2.6 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.35.0-py2.py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 4.5 MB/s eta 0:00:011\n",
      "\u001b[?25h  Downloading tqdm-4.34.0-py2.py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.33.0-py2.py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading tqdm-4.32.2-py2.py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 4.0 MB/s eta 0:00:011\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of tqdm to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
      "INFO: pip is looking at multiple versions of pathlib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pathlib2>=2.3.2\n",
      "  Downloading pathlib2-2.3.4-py2.py3-none-any.whl (18 kB)\n",
      "  Downloading pathlib2-2.3.3-py2.py3-none-any.whl (16 kB)\n",
      "  Downloading pathlib2-2.3.2-py2.py3-none-any.whl (16 kB)\n",
      "INFO: pip is looking at multiple versions of sklearn to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of sentencepiece to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of <Python from Requires-Python> to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of pyzmq to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of jieba to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install -r ERNIE/requirements.txt (line 7), -r ERNIE/requirements.txt (line 8) and six==1.11.0 because these package versions have conflicting dependencies.\u001b[0m\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.1.1 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.1.0 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.5 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.4 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.3 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.2 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.1 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.0 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.0b8 depends on six>=1.14.0\n",
      "    The user requested six==1.11.0\n",
      "    pathlib2 2.3.2 depends on six\n",
      "    visualdl 2.0.0b7 depends on six>=1.14.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ERNIE/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./ERNIE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle as P\n",
    "import paddle.fluid as F\n",
    "import paddle.fluid.layers as L\n",
    "import paddle.fluid.dygraph as D\n",
    "\n",
    "from ernie.tokenizing_ernie import ErnieTokenizer\n",
    "from ernie.modeling_ernie import ErnieModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置好所有的超参数，对于ERNIE任务学习率推荐取1e-5/2e-5/5e-5, 根据显存大小调节BATCH大小, 最大句子长度不超过512.\n",
    "BATCH=32\n",
    "MAX_SEQLEN=128\n",
    "LR=5e-5\n",
    "EPOCH=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 09:56:51,230 - INFO - get pretrain dir from https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz\n",
      "downloading https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz: 788478KB [00:26, 29417.45KB/s]                            \n"
     ]
    }
   ],
   "source": [
    "tokenizer = ErnieTokenizer.from_pretrained('ernie-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17964"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我第一次入住如家，实际情况与现实相差太远，屋子里味道很大，打开窗户换气又很吵，卫生不敢恭维！入住时间不长就搬走。而且位置也很偏，不知道为何在这里选址！总之不会再住！楼下涮肉一般！'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ernie_data(tokenizer, texts, labels):\n",
    "    assert isinstance(texts, list) == True\n",
    "    data = []\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        text_id, _ = tokenizer.encode(text) # ErnieTokenizer 会自动添加ERNIE所需要的特殊token，如[CLS], [SEP]\n",
    "        text_id = text_id[:MAX_SEQLEN]\n",
    "        text_id = np.pad(text_id, [0, MAX_SEQLEN-len(text_id)], mode='constant')\n",
    "        label = labels[i]\n",
    "        data.append((text_id, label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.72 s, sys: 21.3 ms, total: 2.74 s\n",
      "Wall time: 2.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ernie_data_tr = generate_ernie_data(tokenizer, train_texts, label_tr)\n",
    "ernie_data_va = generate_ernie_data(tokenizer, val_texts, label_va)\n",
    "ernie_data_ts = generate_ernie_data(tokenizer, test_texts, label_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-20 10:26:19,049 - INFO - get pretrain dir from https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz\n",
      "2021-03-20 10:26:21,976 - INFO - loading pretrained model from /Users/wegzheng/.paddle-ernie-cache/466eabcffd6d6a83ae9cb97dd1a167bd\n",
      "./ERNIE/ernie/modeling_ernie.py:265: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  log.warn('param:%s not set in pretrained model, skip' % k)\n",
      "2021-03-20 10:26:24,507 - WARNING - param:classifier.weight not set in pretrained model, skip\n",
      "2021-03-20 10:26:24,508 - WARNING - param:classifier.bias not set in pretrained model, skip\n"
     ]
    }
   ],
   "source": [
    "D.guard().__enter__() # 为了让Paddle进入动态图模式，需要添加这一行在最前面\n",
    "\n",
    "ernie = ErnieModelForSequenceClassification.from_pretrained('ernie-1.0', num_labels=3)\n",
    "optimizer = F.optimizer.Adam(LR, parameter_list=ernie.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_data(data, i):\n",
    "    d = data[i*BATCH: (i + 1) * BATCH]\n",
    "    feature, label = zip(*d)\n",
    "    feature = np.stack(feature)  # 将BATCH行样本整合在一个numpy.array中\n",
    "    label = np.stack(list(label))\n",
    "    feature = D.to_variable(feature) # 使用to_variable将numpy.array转换为paddle tensor\n",
    "    label = D.to_variable(label)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ernie_train(train_data, test_data):\n",
    "    for i in range(EPOCH):\n",
    "        np.random.shuffle(train_data) # 每个epoch都shuffle数据以获得最佳训练效果；\n",
    "        #train\n",
    "        for j in range(len(train_data) // BATCH):\n",
    "            feature, label = get_batch_data(train_data, j)\n",
    "            loss, _ = ernie(feature, labels=label) # ernie模型的返回值包含(loss, logits)；其中logits目前暂时不需要使用\n",
    "            loss.backward()\n",
    "            optimizer.minimize(loss)\n",
    "            ernie.clear_gradients()\n",
    "            if j % 2 == 0:\n",
    "                print('train %d: loss %.5f' % (j, loss.numpy()))\n",
    "            # evaluate\n",
    "            if j % 10 == 0:\n",
    "                all_pred, all_label = [], []\n",
    "                with D.base._switch_tracer_mode_guard_(is_train=False): # 在这个with域内ernie不会进行梯度计算；\n",
    "                    ernie.eval() # 控制模型进入eval模式，这将会关闭所有的dropout；\n",
    "                    for j in range(len(test_data) // BATCH):\n",
    "                        feature, label = get_batch_data(test_data, j)\n",
    "                        loss, logits = ernie(feature, labels=label) \n",
    "                        all_pred.extend(L.argmax(logits, -1).numpy())\n",
    "                        all_label.extend(label.numpy())\n",
    "                    ernie.train()\n",
    "                f1 = f1_score(all_label, all_pred, average='macro')\n",
    "                print('f1 %.5f' % f1)\n",
    "    return ernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0: loss 0.71942\n",
      "f1 0.40860\n",
      "train 2: loss 1.06447\n",
      "train 4: loss 0.56342\n",
      "train 6: loss 0.75074\n",
      "train 8: loss 0.68717\n",
      "train 10: loss 0.47982\n",
      "f1 0.43173\n",
      "train 12: loss 0.52679\n",
      "train 14: loss 0.49214\n",
      "train 16: loss 0.52026\n",
      "train 18: loss 0.47697\n",
      "train 20: loss 0.39963\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-68299174c2ce>\u001b[0m in \u001b[0;36mernie_train\u001b[0;34m(train_data, test_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mernie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                         \u001b[0mall_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                         \u001b[0mall_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python_Projects/97_learn/ERNIE/ernie/modeling_ernie.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         pooled, encoded = super(ErnieModelForSequenceClassification,\n\u001b[0;32m--> 466\u001b[0;31m                                 self).forward(*args, **kwargs)\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python_Projects/97_learn/ERNIE/ernie/modeling_ernie.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_ids, sent_ids, pos_ids, input_mask, attn_bias, past_cache, use_causal_mask)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         encoded, hidden_list, cache_list = self.encoder_stack(\n\u001b[0;32m--> 417\u001b[0;31m             embedded, attn_bias, past_cache=past_cache)\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mpooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python_Projects/97_learn/ERNIE/ernie/modeling_ernie.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, attn_bias, past_cache)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0mcache_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mcache_list_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python_Projects/97_learn/ERNIE/ernie/modeling_ernie.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, attn_bias, past_cache)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# dropout/ add/ norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mffn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mffn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffn_out\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python_Projects/97_learn/ERNIE/ernie/modeling_ernie.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_built\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mforward_post_hook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_post_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/nn/layer/common.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         out = F.linear(\n\u001b[0;32m--> 137\u001b[0;31m             x=input, weight=self.weight, bias=self.bias, name=self.name)\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.9/lib/python3.7/site-packages/paddle/nn/functional/common.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(x, weight, bias, name)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \u001b[0mpre_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_varbase_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m         core.ops.matmul(x, weight, pre_bias, 'transpose_X', False,\n\u001b[0;32m-> 1473\u001b[0;31m                         'transpose_Y', False, \"alpha\", 1)\n\u001b[0m\u001b[1;32m   1474\u001b[0m         return dygraph_utils._append_bias_in_dygraph(\n\u001b[1;32m   1475\u001b[0m             pre_bias, bias, axis=len(x.shape) - 1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time ernie_finetune = ernie_train(ernie_data_tr, ernie_data_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 self defined network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_defined_model(tk_length, dim=32, nlabels=1, LR=1e-4):\n",
    "    model_nn = tf.keras.Sequential([\n",
    "    layers.Embedding(tk_length + 1, dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_nn.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "             metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.5))\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ernie_ids(tokenizer, texts, labels):\n",
    "    assert isinstance(texts, list) == True\n",
    "    ids = []\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        text_id, _ = tokenizer.encode(text) # ErnieTokenizer 会自动添加ERNIE所需要的特殊token，如[CLS], [SEP]\n",
    "        text_id = text_id[:MAX_SEQLEN]\n",
    "        text_id = np.pad(text_id, [0, MAX_SEQLEN-len(text_id)], mode='constant')\n",
    "        label = labels[i]\n",
    "        ids.append(text_id)\n",
    "    return np.array(ids), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.65 s, sys: 21.8 ms, total: 2.68 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ernie_ids_tr, label_tr = generate_ernie_ids(tokenizer, train_texts, train_df['label'].values)\n",
    "ernie_ids_va, label_va = generate_ernie_ids(tokenizer, val_texts, valid_df['label'].values)\n",
    "ernie_ids_ts, label_ts = generate_ernie_ids(tokenizer, test_texts, test_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baidu = self_defined_model(tk_length=len(tokenizer.vocab), LR=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 32)          574880    \n",
      "_________________________________________________________________\n",
      "dropout_60 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_61 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 574,913\n",
      "Trainable params: 574,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_baidu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2850 - binary_accuracy: 0.8878 - val_loss: 0.3559 - val_binary_accuracy: 0.8367\n",
      "Epoch 2/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2832 - binary_accuracy: 0.8862 - val_loss: 0.3555 - val_binary_accuracy: 0.8360\n",
      "Epoch 3/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2843 - binary_accuracy: 0.8870 - val_loss: 0.3551 - val_binary_accuracy: 0.8360\n",
      "Epoch 4/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2822 - binary_accuracy: 0.8880 - val_loss: 0.3547 - val_binary_accuracy: 0.8373\n",
      "Epoch 5/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2813 - binary_accuracy: 0.8884 - val_loss: 0.3543 - val_binary_accuracy: 0.8380\n",
      "Epoch 6/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2813 - binary_accuracy: 0.8882 - val_loss: 0.3540 - val_binary_accuracy: 0.8367\n",
      "Epoch 7/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2805 - binary_accuracy: 0.8892 - val_loss: 0.3537 - val_binary_accuracy: 0.8380\n",
      "Epoch 8/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2801 - binary_accuracy: 0.8868 - val_loss: 0.3534 - val_binary_accuracy: 0.8380\n",
      "Epoch 9/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2790 - binary_accuracy: 0.8876 - val_loss: 0.3531 - val_binary_accuracy: 0.8380\n",
      "Epoch 10/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2788 - binary_accuracy: 0.8924 - val_loss: 0.3528 - val_binary_accuracy: 0.8380\n",
      "Epoch 11/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2770 - binary_accuracy: 0.8892 - val_loss: 0.3526 - val_binary_accuracy: 0.8360\n",
      "Epoch 12/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2764 - binary_accuracy: 0.8876 - val_loss: 0.3522 - val_binary_accuracy: 0.8360\n",
      "Epoch 13/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2753 - binary_accuracy: 0.8890 - val_loss: 0.3519 - val_binary_accuracy: 0.8367\n",
      "Epoch 14/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2748 - binary_accuracy: 0.8910 - val_loss: 0.3516 - val_binary_accuracy: 0.8360\n",
      "Epoch 15/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2745 - binary_accuracy: 0.8902 - val_loss: 0.3513 - val_binary_accuracy: 0.8353\n",
      "Epoch 16/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2738 - binary_accuracy: 0.8886 - val_loss: 0.3510 - val_binary_accuracy: 0.8360\n",
      "Epoch 17/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2720 - binary_accuracy: 0.8922 - val_loss: 0.3508 - val_binary_accuracy: 0.8367\n",
      "Epoch 18/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2729 - binary_accuracy: 0.8906 - val_loss: 0.3505 - val_binary_accuracy: 0.8373\n",
      "Epoch 19/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2715 - binary_accuracy: 0.8940 - val_loss: 0.3502 - val_binary_accuracy: 0.8367\n",
      "Epoch 20/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2702 - binary_accuracy: 0.8920 - val_loss: 0.3499 - val_binary_accuracy: 0.8373\n",
      "Epoch 21/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2705 - binary_accuracy: 0.8930 - val_loss: 0.3497 - val_binary_accuracy: 0.8387\n",
      "Epoch 22/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2687 - binary_accuracy: 0.8958 - val_loss: 0.3493 - val_binary_accuracy: 0.8393\n",
      "Epoch 23/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2692 - binary_accuracy: 0.8918 - val_loss: 0.3490 - val_binary_accuracy: 0.8387\n",
      "Epoch 24/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2684 - binary_accuracy: 0.8942 - val_loss: 0.3487 - val_binary_accuracy: 0.8393\n",
      "Epoch 25/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2680 - binary_accuracy: 0.8944 - val_loss: 0.3485 - val_binary_accuracy: 0.8387\n",
      "Epoch 26/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2657 - binary_accuracy: 0.8936 - val_loss: 0.3482 - val_binary_accuracy: 0.8387\n",
      "Epoch 27/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2660 - binary_accuracy: 0.8968 - val_loss: 0.3482 - val_binary_accuracy: 0.8393\n",
      "Epoch 28/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2657 - binary_accuracy: 0.8948 - val_loss: 0.3480 - val_binary_accuracy: 0.8393\n",
      "Epoch 29/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2643 - binary_accuracy: 0.8950 - val_loss: 0.3476 - val_binary_accuracy: 0.8387\n",
      "Epoch 30/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2648 - binary_accuracy: 0.8942 - val_loss: 0.3474 - val_binary_accuracy: 0.8387\n",
      "Epoch 31/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2637 - binary_accuracy: 0.8960 - val_loss: 0.3472 - val_binary_accuracy: 0.8380\n",
      "Epoch 32/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2636 - binary_accuracy: 0.8948 - val_loss: 0.3469 - val_binary_accuracy: 0.8380\n",
      "Epoch 33/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2623 - binary_accuracy: 0.8952 - val_loss: 0.3467 - val_binary_accuracy: 0.8387\n",
      "Epoch 34/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2622 - binary_accuracy: 0.8976 - val_loss: 0.3465 - val_binary_accuracy: 0.8387\n",
      "Epoch 35/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2608 - binary_accuracy: 0.8960 - val_loss: 0.3464 - val_binary_accuracy: 0.8387\n",
      "Epoch 36/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2620 - binary_accuracy: 0.8966 - val_loss: 0.3460 - val_binary_accuracy: 0.8393\n",
      "Epoch 37/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2599 - binary_accuracy: 0.8966 - val_loss: 0.3460 - val_binary_accuracy: 0.8393\n",
      "Epoch 38/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2598 - binary_accuracy: 0.8966 - val_loss: 0.3458 - val_binary_accuracy: 0.8393\n",
      "Epoch 39/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2581 - binary_accuracy: 0.8988 - val_loss: 0.3456 - val_binary_accuracy: 0.8400\n",
      "Epoch 40/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2574 - binary_accuracy: 0.8986 - val_loss: 0.3454 - val_binary_accuracy: 0.8400\n",
      "Epoch 41/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2572 - binary_accuracy: 0.8966 - val_loss: 0.3452 - val_binary_accuracy: 0.8407\n",
      "Epoch 42/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2571 - binary_accuracy: 0.9004 - val_loss: 0.3450 - val_binary_accuracy: 0.8413\n",
      "Epoch 43/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2559 - binary_accuracy: 0.8986 - val_loss: 0.3448 - val_binary_accuracy: 0.8420\n",
      "Epoch 44/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2541 - binary_accuracy: 0.9014 - val_loss: 0.3447 - val_binary_accuracy: 0.8413\n",
      "Epoch 45/200\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.2555 - binary_accuracy: 0.9008 - val_loss: 0.3445 - val_binary_accuracy: 0.8413\n",
      "Epoch 46/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2533 - binary_accuracy: 0.9008 - val_loss: 0.3444 - val_binary_accuracy: 0.8427\n",
      "Epoch 47/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2545 - binary_accuracy: 0.8994 - val_loss: 0.3442 - val_binary_accuracy: 0.8427\n",
      "Epoch 48/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2537 - binary_accuracy: 0.8996 - val_loss: 0.3440 - val_binary_accuracy: 0.8427\n",
      "Epoch 49/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2527 - binary_accuracy: 0.9016 - val_loss: 0.3439 - val_binary_accuracy: 0.8433\n",
      "Epoch 50/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2521 - binary_accuracy: 0.8992 - val_loss: 0.3438 - val_binary_accuracy: 0.8440\n",
      "Epoch 51/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2512 - binary_accuracy: 0.9020 - val_loss: 0.3436 - val_binary_accuracy: 0.8433\n",
      "Epoch 52/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2513 - binary_accuracy: 0.8992 - val_loss: 0.3434 - val_binary_accuracy: 0.8433\n",
      "Epoch 53/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2506 - binary_accuracy: 0.9018 - val_loss: 0.3433 - val_binary_accuracy: 0.8433\n",
      "Epoch 54/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2503 - binary_accuracy: 0.8992 - val_loss: 0.3432 - val_binary_accuracy: 0.8420\n",
      "Epoch 55/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2484 - binary_accuracy: 0.9008 - val_loss: 0.3430 - val_binary_accuracy: 0.8427\n",
      "Epoch 56/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2492 - binary_accuracy: 0.9010 - val_loss: 0.3430 - val_binary_accuracy: 0.8433\n",
      "Epoch 57/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2490 - binary_accuracy: 0.9000 - val_loss: 0.3427 - val_binary_accuracy: 0.8440\n",
      "Epoch 58/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2473 - binary_accuracy: 0.9032 - val_loss: 0.3426 - val_binary_accuracy: 0.8427\n",
      "Epoch 59/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2482 - binary_accuracy: 0.9010 - val_loss: 0.3426 - val_binary_accuracy: 0.8433\n",
      "Epoch 60/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2475 - binary_accuracy: 0.9016 - val_loss: 0.3425 - val_binary_accuracy: 0.8433\n",
      "Epoch 61/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2464 - binary_accuracy: 0.9028 - val_loss: 0.3423 - val_binary_accuracy: 0.8427\n",
      "Epoch 62/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2453 - binary_accuracy: 0.9042 - val_loss: 0.3422 - val_binary_accuracy: 0.8440\n",
      "Epoch 63/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2441 - binary_accuracy: 0.9018 - val_loss: 0.3421 - val_binary_accuracy: 0.8427\n",
      "Epoch 64/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2461 - binary_accuracy: 0.9032 - val_loss: 0.3420 - val_binary_accuracy: 0.8447\n",
      "Epoch 65/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2434 - binary_accuracy: 0.9052 - val_loss: 0.3420 - val_binary_accuracy: 0.8447\n",
      "Epoch 66/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2434 - binary_accuracy: 0.9048 - val_loss: 0.3418 - val_binary_accuracy: 0.8440\n",
      "Epoch 67/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2431 - binary_accuracy: 0.9030 - val_loss: 0.3417 - val_binary_accuracy: 0.8453\n",
      "Epoch 68/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2434 - binary_accuracy: 0.9046 - val_loss: 0.3416 - val_binary_accuracy: 0.8453\n",
      "Epoch 69/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2423 - binary_accuracy: 0.9044 - val_loss: 0.3417 - val_binary_accuracy: 0.8453\n",
      "Epoch 70/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2407 - binary_accuracy: 0.9044 - val_loss: 0.3415 - val_binary_accuracy: 0.8460\n",
      "Epoch 71/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2405 - binary_accuracy: 0.9060 - val_loss: 0.3414 - val_binary_accuracy: 0.8460\n",
      "Epoch 72/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2405 - binary_accuracy: 0.9040 - val_loss: 0.3415 - val_binary_accuracy: 0.8447\n",
      "Epoch 73/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2394 - binary_accuracy: 0.9056 - val_loss: 0.3414 - val_binary_accuracy: 0.8447\n",
      "Epoch 74/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2398 - binary_accuracy: 0.9060 - val_loss: 0.3412 - val_binary_accuracy: 0.8453\n",
      "Epoch 75/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2399 - binary_accuracy: 0.9048 - val_loss: 0.3413 - val_binary_accuracy: 0.8447\n",
      "Epoch 76/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2389 - binary_accuracy: 0.9064 - val_loss: 0.3412 - val_binary_accuracy: 0.8467\n",
      "Epoch 77/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2375 - binary_accuracy: 0.9060 - val_loss: 0.3411 - val_binary_accuracy: 0.8473\n",
      "Epoch 78/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2369 - binary_accuracy: 0.9066 - val_loss: 0.3411 - val_binary_accuracy: 0.8460\n",
      "Epoch 79/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2371 - binary_accuracy: 0.9066 - val_loss: 0.3411 - val_binary_accuracy: 0.8460\n",
      "Epoch 80/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2368 - binary_accuracy: 0.9064 - val_loss: 0.3410 - val_binary_accuracy: 0.8473\n",
      "Epoch 81/200\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.2363 - binary_accuracy: 0.9066 - val_loss: 0.3408 - val_binary_accuracy: 0.8467\n",
      "Epoch 82/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2358 - binary_accuracy: 0.9060 - val_loss: 0.3407 - val_binary_accuracy: 0.8473\n",
      "Epoch 83/200\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.2348 - binary_accuracy: 0.905 - 0s 5ms/step - loss: 0.2344 - binary_accuracy: 0.9066 - val_loss: 0.3408 - val_binary_accuracy: 0.8473\n",
      "Epoch 84/200\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.2335 - binary_accuracy: 0.9084 - val_loss: 0.3408 - val_binary_accuracy: 0.8460\n",
      "CPU times: user 1min 38s, sys: 15.6 s, total: 1min 53s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the model\n",
    "epochs = 400\n",
    "history = model_baidu.fit(ernie_ids_tr, label_tr,\n",
    "                       batch_size=64, \n",
    "                       epochs=epochs, \n",
    "                       validation_data=(ernie_ids_va, label_va),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8862559241706162"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_baidu.predict_classes(ernie_ids_ts)\n",
    "accuracy_score(label_ts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_lstm_model(tk_length, dim=32, nlabels=1, LR=1e-4):\n",
    "    model_nn = tf.keras.Sequential([\n",
    "    layers.Embedding(tk_length + 1, dim),\n",
    "    LSTM(100, dropout=0.3, recurrent_dropout=0.3),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model_nn.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "             metrics=tf.keras.metrics.BinaryAccuracy(threshold=0.5))\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = self_lstm_model(tk_length=len(tokenizer.vocab), LR=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "79/79 [==============================] - 10s 103ms/step - loss: 0.6865 - binary_accuracy: 0.3009 - val_loss: 0.6735 - val_binary_accuracy: 0.3227\n",
      "Epoch 2/300\n",
      "79/79 [==============================] - 7s 95ms/step - loss: 0.6674 - binary_accuracy: 0.3201 - val_loss: 0.6376 - val_binary_accuracy: 0.3227\n",
      "Epoch 3/300\n",
      "79/79 [==============================] - 8s 97ms/step - loss: 0.6181 - binary_accuracy: 0.5437 - val_loss: 0.6106 - val_binary_accuracy: 0.6327\n",
      "Epoch 4/300\n",
      "79/79 [==============================] - 8s 95ms/step - loss: 0.6030 - binary_accuracy: 0.6342 - val_loss: 0.6103 - val_binary_accuracy: 0.6353\n",
      "Epoch 5/300\n",
      "79/79 [==============================] - 7s 94ms/step - loss: 0.6069 - binary_accuracy: 0.6334 - val_loss: 0.6098 - val_binary_accuracy: 0.6347\n",
      "Epoch 6/300\n",
      "79/79 [==============================] - 8s 103ms/step - loss: 0.6126 - binary_accuracy: 0.6294 - val_loss: 0.6100 - val_binary_accuracy: 0.6380\n",
      "Epoch 7/300\n",
      "79/79 [==============================] - 9s 111ms/step - loss: 0.6138 - binary_accuracy: 0.6248 - val_loss: 0.6091 - val_binary_accuracy: 0.6373\n",
      "Epoch 8/300\n",
      "79/79 [==============================] - 9s 113ms/step - loss: 0.6070 - binary_accuracy: 0.6231 - val_loss: 0.6087 - val_binary_accuracy: 0.6373\n",
      "Epoch 9/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.5996 - binary_accuracy: 0.6472 - val_loss: 0.6085 - val_binary_accuracy: 0.6373\n",
      "Epoch 10/300\n",
      "79/79 [==============================] - 9s 112ms/step - loss: 0.6043 - binary_accuracy: 0.6297 - val_loss: 0.6077 - val_binary_accuracy: 0.6400\n",
      "Epoch 11/300\n",
      "79/79 [==============================] - 9s 110ms/step - loss: 0.6062 - binary_accuracy: 0.6387 - val_loss: 0.6069 - val_binary_accuracy: 0.6393\n",
      "Epoch 12/300\n",
      "79/79 [==============================] - 9s 113ms/step - loss: 0.5992 - binary_accuracy: 0.6444 - val_loss: 0.6061 - val_binary_accuracy: 0.6433\n",
      "Epoch 13/300\n",
      "79/79 [==============================] - 9s 111ms/step - loss: 0.6033 - binary_accuracy: 0.6441 - val_loss: 0.6051 - val_binary_accuracy: 0.6507\n",
      "Epoch 14/300\n",
      "79/79 [==============================] - 9s 111ms/step - loss: 0.5988 - binary_accuracy: 0.6508 - val_loss: 0.6038 - val_binary_accuracy: 0.6747\n",
      "Epoch 15/300\n",
      "79/79 [==============================] - 9s 113ms/step - loss: 0.5966 - binary_accuracy: 0.6810 - val_loss: 0.6014 - val_binary_accuracy: 0.6973\n",
      "Epoch 16/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.5924 - binary_accuracy: 0.7233 - val_loss: 0.5982 - val_binary_accuracy: 0.6607\n",
      "Epoch 17/300\n",
      "79/79 [==============================] - 9s 115ms/step - loss: 0.5995 - binary_accuracy: 0.6839 - val_loss: 0.5839 - val_binary_accuracy: 0.7547\n",
      "Epoch 18/300\n",
      "79/79 [==============================] - 9s 113ms/step - loss: 0.5539 - binary_accuracy: 0.7707 - val_loss: 0.4735 - val_binary_accuracy: 0.7613\n",
      "Epoch 19/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.4305 - binary_accuracy: 0.7978 - val_loss: 0.4478 - val_binary_accuracy: 0.8060\n",
      "Epoch 20/300\n",
      "79/79 [==============================] - 9s 113ms/step - loss: 0.4069 - binary_accuracy: 0.8211 - val_loss: 0.4323 - val_binary_accuracy: 0.8093\n",
      "Epoch 21/300\n",
      "79/79 [==============================] - 9s 112ms/step - loss: 0.3952 - binary_accuracy: 0.8230 - val_loss: 0.4222 - val_binary_accuracy: 0.8160\n",
      "Epoch 22/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.3770 - binary_accuracy: 0.8333 - val_loss: 0.4153 - val_binary_accuracy: 0.8180\n",
      "Epoch 23/300\n",
      "79/79 [==============================] - 9s 115ms/step - loss: 0.3589 - binary_accuracy: 0.8453 - val_loss: 0.4057 - val_binary_accuracy: 0.8300\n",
      "Epoch 24/300\n",
      "79/79 [==============================] - 9s 116ms/step - loss: 0.3377 - binary_accuracy: 0.8489 - val_loss: 0.3957 - val_binary_accuracy: 0.8187\n",
      "Epoch 25/300\n",
      "79/79 [==============================] - 9s 116ms/step - loss: 0.3413 - binary_accuracy: 0.8549 - val_loss: 0.3958 - val_binary_accuracy: 0.8380\n",
      "Epoch 26/300\n",
      "79/79 [==============================] - 9s 120ms/step - loss: 0.3405 - binary_accuracy: 0.8571 - val_loss: 0.3828 - val_binary_accuracy: 0.8360\n",
      "Epoch 27/300\n",
      "79/79 [==============================] - 10s 123ms/step - loss: 0.3226 - binary_accuracy: 0.8678 - val_loss: 0.3810 - val_binary_accuracy: 0.8360\n",
      "Epoch 28/300\n",
      "79/79 [==============================] - 10s 120ms/step - loss: 0.3106 - binary_accuracy: 0.8632 - val_loss: 0.3855 - val_binary_accuracy: 0.8433\n",
      "Epoch 29/300\n",
      "79/79 [==============================] - 9s 116ms/step - loss: 0.3107 - binary_accuracy: 0.8697 - val_loss: 0.3782 - val_binary_accuracy: 0.8413\n",
      "Epoch 30/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.2979 - binary_accuracy: 0.8747 - val_loss: 0.3718 - val_binary_accuracy: 0.8340\n",
      "Epoch 31/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.2968 - binary_accuracy: 0.8721 - val_loss: 0.3816 - val_binary_accuracy: 0.8467\n",
      "Epoch 32/300\n",
      "79/79 [==============================] - 9s 114ms/step - loss: 0.2752 - binary_accuracy: 0.8853 - val_loss: 0.3659 - val_binary_accuracy: 0.8480\n",
      "Epoch 33/300\n",
      "79/79 [==============================] - 9s 113ms/step - loss: 0.2733 - binary_accuracy: 0.8808 - val_loss: 0.3538 - val_binary_accuracy: 0.8433\n",
      "Epoch 34/300\n",
      "79/79 [==============================] - 9s 117ms/step - loss: 0.2730 - binary_accuracy: 0.8874 - val_loss: 0.3784 - val_binary_accuracy: 0.8240\n",
      "Epoch 35/300\n",
      "79/79 [==============================] - 9s 119ms/step - loss: 0.2726 - binary_accuracy: 0.8792 - val_loss: 0.3566 - val_binary_accuracy: 0.8347\n",
      "CPU times: user 21min 46s, sys: 10min 7s, total: 31min 53s\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fit the model\n",
    "epochs = 300\n",
    "history = model_lstm.fit(ernie_ids_tr, label_tr,\n",
    "                       batch_size=64, \n",
    "                       epochs=epochs, \n",
    "                       validation_data=(ernie_ids_va, label_va),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8720379146919431"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_lstm.predict_classes(ernie_ids_ts)\n",
    "accuracy_score(label_ts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
