{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the standard working process with NLP chinese corpus?\n",
    "* What is the difference between BERT chinese and baidu ERNIE?\n",
    "* How does tokenizer work and the relation with embedding layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./ERNIE')\n",
    "from ernie.tokenizing_ernie import ErnieTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bert_ids(tokenizer, texts, labels, output_length=128):\n",
    "    bert_ids = tokenizer(texts, padding=True, truncation=True, return_tensors=\"tf\", max_length=output_length)\n",
    "    input_ids = np.asarray(bert_ids['input_ids'])\n",
    "    attention_mask = np.asarray(bert_ids['attention_mask'])\n",
    "    return input_ids, attention_mask, np.array(labels), bert_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(target, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ChineseNlpCorpus/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7766, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5322\n",
       "0    2444\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], np.array(df['label']), test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4348,)\n",
      "(1088,)\n",
      "(2330,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 BERT Chinese\n",
    "###  tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "max_length = 128  # Maximum length of input sentence to the model.\n",
    "max_features = 20000  # Only consider the top 20k words\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='auto', restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at /Users/wegzheng/Downloads/bert-base-chinses and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\", model_max_length=5000)\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('/Users/wegzheng/Downloads/bert-base-chinses',num_labels=2)\n",
    "# bert_model = transformers.TFBertModel.from_pretrained(\"/Users/wegzheng/Downloads/bert-base-chinses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary sieze\n",
    "bert_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming texts to ids\n",
    "# %%time\n",
    "input_ids_tr, att_mask_tr, label_tr, tr_dict = generate_bert_ids(bert_tokenizer, X_train.astype('str').to_list(), y_train)\n",
    "input_ids_va, att_mask_va, label_va, va_dict = generate_bert_ids(bert_tokenizer, X_val.astype('str').to_list(), y_val)\n",
    "input_ids_ts, att_mask_ts, label_ts, ts_dict = generate_bert_ids(bert_tokenizer, X_test.astype('str').to_list(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4348\n",
      "1088\n",
      "2330\n"
     ]
    }
   ],
   "source": [
    "print(len(label_tr))\n",
    "print(len(label_va))\n",
    "print(len(label_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(300))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300)               387000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 2,947,301\n",
      "Trainable params: 2,947,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 0.6011 - accuracy: 0.6978 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 0.5869 - accuracy: 0.7284 - val_loss: 0.6136 - val_accuracy: 0.6774\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 20s 148ms/step - loss: 0.5497 - accuracy: 0.7445 - val_loss: 0.6250 - val_accuracy: 0.6792\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 20s 146ms/step - loss: 0.5214 - accuracy: 0.7647 - val_loss: 0.6536 - val_accuracy: 0.6857\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 21s 157ms/step - loss: 0.4868 - accuracy: 0.7829 - val_loss: 0.7068 - val_accuracy: 0.6884\n",
      "CPU times: user 7min 19s, sys: 2min 49s, total: 10min 8s\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a8f2b310>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(input_ids_tr, label_tr, \n",
    "          batch_size=32, \n",
    "          epochs=100, \n",
    "          validation_data=(input_ids_va, label_va),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.81 s, sys: 3.77 s, total: 13.6 s\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%time preds = model.predict(input_ids_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.688412017167382\n",
      "0.614281247734358\n"
     ]
    }
   ],
   "source": [
    "pred_labels = [1 if i >=0.5 else 0 for i in pd.DataFrame(preds)[0].to_list()]\n",
    "print(accuracy_score(label_ts, pred_labels))\n",
    "print(roc_auc(label_ts, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 self-defined BI-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,757,761\n",
      "Trainable params: 2,757,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/136 [==============================] - 21s 121ms/step - loss: 0.5595 - accuracy: 0.7039 - val_loss: 0.3734 - val_accuracy: 0.8382\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 15s 112ms/step - loss: 0.2842 - accuracy: 0.8812 - val_loss: 0.3345 - val_accuracy: 0.8539\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 15s 112ms/step - loss: 0.2049 - accuracy: 0.9224 - val_loss: 0.3553 - val_accuracy: 0.8502\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 15s 113ms/step - loss: 0.1694 - accuracy: 0.9385 - val_loss: 0.3656 - val_accuracy: 0.8483\n",
      "CPU times: user 5min, sys: 1min 59s, total: 7min\n",
      "Wall time: 1min 6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abaccc10>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(input_ids_tr, label_tr, \n",
    "          batch_size=32, \n",
    "          epochs=100, \n",
    "          validation_data=(input_ids_va, label_va),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.37 s, sys: 2.48 s, total: 7.84 s\n",
      "Wall time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "%time preds = model.predict(input_ids_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [1 if i >=0.5 else 0 for i in pd.DataFrame(preds)[0].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8523605150214593"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(label_ts, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8295590753424658"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc(label_ts, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 BERT chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  102267648 \n",
      "_________________________________________________________________\n",
      "dropout_334 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 102,269,186\n",
      "Trainable params: 102,269,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Bert Model None\n"
     ]
    }
   ],
   "source": [
    "bert_model.trainable = True\n",
    "print('\\nBert Model',bert_model.summary())\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-04)\n",
    "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "\n",
    "# loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "# bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wegzheng/.pyenv/versions/3.7.9/lib/python3.7/logging/__init__.py:8: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "  # not be used in advertising or publicity pertaining to distribution\n",
      "2021-03-22 15:22:45,532 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 15:22:45,550 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 15:22:48,861 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 15:22:48,882 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - ETA: 0s - loss: 0.4616 - accuracy: 0.7552 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 15:47:33,157 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 15:47:33,180 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1626s 12s/step - loss: 0.4607 - accuracy: 0.7558 - val_loss: 0.2655 - val_accuracy: 0.8961\n",
      "Epoch 2/3\n",
      "136/136 [==============================] - 1622s 12s/step - loss: 0.2183 - accuracy: 0.9140 - val_loss: 0.2427 - val_accuracy: 0.9072\n",
      "Epoch 3/3\n",
      "136/136 [==============================] - 1617s 12s/step - loss: 0.1504 - accuracy: 0.9501 - val_loss: 0.3048 - val_accuracy: 0.8915\n",
      "CPU times: user 9h 15min 43s, sys: 2h 26min 23s, total: 11h 42min 7s\n",
      "Wall time: 1h 21min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = bert_model.fit([input_ids_tr,att_mask_tr],label_tr,\n",
    "                       batch_size=32, \n",
    "                       epochs=3, \n",
    "                       validation_data=([input_ids_va,att_mask_va],label_va),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 16:49:19,647 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 16:49:19,685 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28min 36s, sys: 9min 5s, total: 37min 41s\n",
      "Wall time: 4min 48s\n"
     ]
    }
   ],
   "source": [
    "%time preds = bert_model.predict([input_ids_ts,att_mask_ts],batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9072961373390558"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arr = tf.nn.softmax(preds[0], axis=-1)\n",
    "pred_labels = tf.argmax(pred_arr, axis=1).numpy()\n",
    "accuracy_score(label_ts, pred_labels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8976139225368956"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc(label_ts, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model.save_pretrained('model/finetuned/bert-base-chinese/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 baidu ERNIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 17:06:57,384 - INFO - get pretrain dir from https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "tokenizer_baidu = ErnieTokenizer.from_pretrained('ernie-1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['房间很大，服务也很好，地理位置比较好，但宾馆门前停车很不方便，比较窄。',\n",
       " '酒店非常的好，位子也还不错。楼上的餐厅可以看到很美的上海夜景非常喜欢，价格太高啦',\n",
       " '本想订汉庭，地理位置不错的酒店，从汽车西站叫车过去15元，离文昌阁几步之遥，10分钟可到有名的福满楼，金鹰国际购物中心、时代广场近在咫尺。个园、何园、瘦西湖步行20分钟内均可到达，三轮的话5元没问题。大堂宽敞，房间设施符合三星要求，床铺整洁。只是卫生间略显成旧，用的是浴缸，不过坐厕、龙头都是美标的，水也不是想象中那么小、凉，总体还算不错。10元的早餐内容较少，不过咱也不图这个对吧！哈哈，下次去还订那儿。补充点评2008年5月13日：本想订汉庭，预定已满订了红杉树。地理位置不错的酒店，从汽车西站叫车过去15元，离文昌阁几步之遥，10分钟可到有名的福满楼，金鹰国际购物中心、时代广场近在咫尺。个园、何园、瘦西湖步行20分钟内均可到达，三轮的话5元没问题。大堂宽敞，房间设施符合三星要求，床铺整洁。只是卫生间略显成旧，用的是浴缸，不过坐厕、龙头都是美标的，水也不是想象中那么小、凉，总体还算不错。10元的早餐内容较少，不过咱也不图这个对吧！哈哈，下次去还订那儿。']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.astype('str').to_list()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    1,   458,   143,   321,    19,     4,   231,   112,   105,\n",
       "          321,   170,     4,    31,    38,   144,   521,   277,   420,\n",
       "          170,     4,   255,  1468,   774,   232,   152,  1025,   320,\n",
       "          321,    16,    58,   518,     4,   277,   420,  2325, 12043,\n",
       "            2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_baidu.encode(X_train.astype('str').to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ernie_ids(tokenizer, texts, labels, max_len):\n",
    "    assert isinstance(texts, list) == True\n",
    "    ids = []\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        text_id, _ = tokenizer.encode(text) # ErnieTokenizer 会自动添加ERNIE所需要的特殊token，如[CLS], [SEP]\n",
    "        text_id = text_id[:max_len]\n",
    "        text_id = np.pad(text_id, [0, max_len-len(text_id)], mode='constant')\n",
    "        label = labels[i]\n",
    "        ids.append(text_id)\n",
    "    return np.array(ids), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.82 s, sys: 14 ms, total: 2.84 s\n",
      "Wall time: 2.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ernie_ids_tr, label_tr = generate_ernie_ids(tokenizer_baidu, X_train.astype('str').to_list(), label_tr, max_len=128)\n",
    "ernie_ids_va, label_va = generate_ernie_ids(tokenizer_baidu, X_val.astype('str').to_list(), label_va, max_len=128)\n",
    "ernie_ids_ts, label_ts = generate_ernie_ids(tokenizer_baidu, X_test.astype('str').to_list(), label_ts, max_len=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 finetune with bi-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/136 [==============================] - 22s 124ms/step - loss: 0.5257 - accuracy: 0.7449 - val_loss: 0.3600 - val_accuracy: 0.8392\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 15s 114ms/step - loss: 0.2656 - accuracy: 0.8937 - val_loss: 0.3379 - val_accuracy: 0.8511\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 15s 113ms/step - loss: 0.1890 - accuracy: 0.9289 - val_loss: 0.4144 - val_accuracy: 0.8594\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 16s 117ms/step - loss: 0.1707 - accuracy: 0.9347 - val_loss: 0.4533 - val_accuracy: 0.8392\n",
      "CPU times: user 5min 6s, sys: 2min 3s, total: 7min 9s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1af6a2ed0>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(ernie_ids_tr, label_tr, \n",
    "          batch_size=32, \n",
    "          epochs=100, \n",
    "          validation_data=(ernie_ids_va, label_va),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.99 s, sys: 2.92 s, total: 9.9 s\n",
      "Wall time: 2.6 s\n"
     ]
    }
   ],
   "source": [
    "%time preds = model.predict(ernie_ids_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863519313304721\n",
      "0.8446244618696187\n"
     ]
    }
   ],
   "source": [
    "pred_labels = [1 if i >=0.5 else 0 for i in pd.DataFrame(preds)[0].to_list()]\n",
    "print(accuracy_score(label_ts, pred_labels))\n",
    "print(roc_auc(label_ts, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ernie on distilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ernie_ids_tr & att_mask_tr as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_354']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distil_model = transformers.TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_distil_tr = [ernie_ids_tr, att_mask_tr]\n",
    "input_distil_va = [ernie_ids_va, att_mask_va]\n",
    "input_distil_ts = [ernie_ids_ts, att_mask_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-06)\n",
    "distil_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "68/68 [==============================] - 750s 11s/step - loss: 0.2829 - accuracy: 0.8903 - val_loss: 0.3610 - val_accuracy: 0.8428\n",
      "Epoch 2/3\n",
      "68/68 [==============================] - 738s 11s/step - loss: 0.2424 - accuracy: 0.9085 - val_loss: 0.3327 - val_accuracy: 0.8621\n",
      "Epoch 3/3\n",
      "68/68 [==============================] - 733s 11s/step - loss: 0.2041 - accuracy: 0.9257 - val_loss: 0.3224 - val_accuracy: 0.8548\n",
      "CPU times: user 4h 22min 38s, sys: 1h 4min 27s, total: 5h 27min 5s\n",
      "Wall time: 37min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = distil_model.fit(input_distil_tr,label_tr,\n",
    "                       batch_size=64, \n",
    "                       epochs=3, \n",
    "                       validation_data=(input_distil_va,label_va),\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   1,  458,  143, ...,    0,    0,    0],\n",
       "        [   1,  661,  737, ...,    0,    0,    0],\n",
       "        [   1,   89,  313, ...,  597,   33, 1100],\n",
       "        ...,\n",
       "        [   1,  520,   15, ...,    4,   39,  232],\n",
       "        [   1,   31,   38, ...,    0,    0,    0],\n",
       "        [   1,  836,   97, ...,    0,    0,    0]]),\n",
       " array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 0, 0]], dtype=int32)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_distil_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 20:25:34,138 - WARNING - The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-22 20:25:34,161 - WARNING - The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 52s, sys: 3min 38s, total: 17min 30s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%time preds = distil_model.predict(input_distil_ts,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8545064377682403\n",
      "0.8358126626254221\n"
     ]
    }
   ],
   "source": [
    "pred_arr = tf.nn.softmax(preds[0], axis=-1)\n",
    "pred_labels = tf.argmax(pred_arr, axis=1).numpy()\n",
    "print(accuracy_score(label_ts, pred_labels,))\n",
    "print(roc_auc(label_ts, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ernie on single bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,757,761\n",
      "Trainable params: 2,757,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input for variable-length sequences of integers\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Embed each integer in a 128-dimensional vector\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = layers.Bidirectional(layers.LSTM(64,dropout=0.3, recurrent_dropout=0.3, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# Add a classifier\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model_lstm = tf.keras.Model(inputs, outputs)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "136/136 [==============================] - 30s 174ms/step - loss: 0.6513 - accuracy: 0.6792 - val_loss: 0.5969 - val_accuracy: 0.6866\n",
      "Epoch 2/100\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 0.5559 - accuracy: 0.6921 - val_loss: 0.4264 - val_accuracy: 0.8309\n",
      "Epoch 3/100\n",
      "136/136 [==============================] - 23s 168ms/step - loss: 0.3791 - accuracy: 0.8447 - val_loss: 0.3876 - val_accuracy: 0.8401\n",
      "Epoch 4/100\n",
      "136/136 [==============================] - 23s 167ms/step - loss: 0.2855 - accuracy: 0.8856 - val_loss: 0.3360 - val_accuracy: 0.8511\n",
      "Epoch 5/100\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 0.2431 - accuracy: 0.9135 - val_loss: 0.3405 - val_accuracy: 0.8585\n",
      "Epoch 6/100\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 0.2112 - accuracy: 0.9265 - val_loss: 0.3496 - val_accuracy: 0.8594\n",
      "Epoch 7/100\n",
      "136/136 [==============================] - 23s 169ms/step - loss: 0.1820 - accuracy: 0.9360 - val_loss: 0.3584 - val_accuracy: 0.8631\n",
      "CPU times: user 13min 53s, sys: 5min 10s, total: 19min 4s\n",
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bb403090>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5,epsilon=1e-04)\n",
    "# model_lstm.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_lstm.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model_lstm.fit(ernie_ids_tr, label_tr, \n",
    "          batch_size=32, \n",
    "          epochs=100, \n",
    "          validation_data=(ernie_ids_va, label_va),\n",
    "         callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.51 s, sys: 3.2 s, total: 11.7 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "%time preds = model_lstm.predict(ernie_ids_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8592274678111588\n",
      "0.8351891154022439\n"
     ]
    }
   ],
   "source": [
    "pred_labels = [1 if i >=0.5 else 0 for i in pd.DataFrame(preds)[0].to_list()]\n",
    "print(accuracy_score(label_ts, pred_labels))\n",
    "print(roc_auc(label_ts, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
