{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatQuest Notes\n",
    "> last updated by Wei, Apr 2020\n",
    "\n",
    "## Part 1 Gradient Bossting\n",
    "### Gradient Boost regression\n",
    "#### main ideas\n",
    "- Gradienboost started with an initial value (a single leaf, always can be a mean value), while adaboost started with a weak classifier\n",
    "- a new Tree was built to reduce or represent the __Pseudo Residual__ of previous trees\n",
    "  * Intial leaf -> calculate Residual\n",
    "  * adding First Tree -> calculate leaf output by minimize the leaf loss\n",
    "  * update by calculating Residual\n",
    "- number of leafs can usually be 8 to 32\n",
    "- using learning rate (e.g 0.1) to scale the contribution of a new tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### algorithm\n",
    "- Input data and differential Loss function\n",
    "$$\\{(x_{i}, y_{i})\\}_{i=1} ^ n and L(y_{i}, F(x))$$\n",
    "where mean square error will be used:\n",
    "$$L = \\frac{1}{2}\\sum(y_{i} - p_{i})^2$$\n",
    "- Step 1: Initiallize model with a constant value:\n",
    "    * Here, the initial value is mean\n",
    "$$ F_{0}(x) = argmin \\sum_{i=1}^n L(y_{i}, \\gamma) $$\n",
    "\n",
    "- Step 2: for m = 1 to M:\n",
    "    * (A) Compute residual `r_im`\n",
    "    $$r_{im} = -[\\frac{\\partial L(y_{i}, F(x_{i}))}{\\partial F(x_{i})}]_{F_{x}=F_{m-1}(x)}  for  i=1,...,n $$\n",
    "    * (B) Fit a regression tree to the `r_im` values and create terminal regions `R_jm`, for j=1,..,Jm\n",
    "    * (C) For j = 1...Jm, compute output of each leaf:\n",
    "    $$\\gamma_{jm} = argmin \\sum_{x_{i} \\in R_{ij}}L(y_{i}, F_{m-1}(x_{i}) + \\gamma)$$\n",
    "Mathmatically, it is the average residual:\n",
    "<font color=blue>\n",
    "$$\\gamma = \\frac{\\sum residual}{n obs}$$\n",
    "    </font>\n",
    "    * (D) Update each obs\n",
    "    $$F_{m}(x)=F_{m-1}(x)+\\nu*\\sum_{j=1}^{J_{m}}\\gamma_{im}I (x\\in R_{jm})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting classification\n",
    "#### main idea\n",
    "- started with an initial value __log(odds)__ (e.g log (4 to 2) = 0.7 and transform to p = 2/3\n",
    "- the calculation of residual on leaf node is different from regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### algorithm\n",
    "- Input data and differential Loss function, loss function is negative log likelihood:\n",
    "<font color=blue>\n",
    "$$ -\\sum_{i=1}^Ny_{i}*log(p) + (1-y_{i}) * log(1-p)$$\n",
    "    </font>\n",
    "it can be transformed to:\n",
    "$$ -y_{i}*[log(p) - log(1-p)] - log(1-p)$$\n",
    "next:\n",
    "$$-y_{i}*log\\frac{p}{1-p} - log(1-p)$$\n",
    "since we can prove:\n",
    "$$log\\frac{p}{1-p} = log(odds)$$\n",
    "$$log(1-p) = -log(1+e^{log(odds)}))$$\n",
    "we can finally get our loss function to:\n",
    "$$-y_{i}*log(odds) + log(1+e^{log(odds)})$$\n",
    "\n",
    "Also, we can prove that:\n",
    "$$\\frac{\\partial L}{\\partial log(odds)} = - y_{i} + \\frac{e^{log(odds)}}{1+e^{log(odds)}} = -y_{i} + p$$\n",
    "- Step 1: Initiallize model with a constant value:\n",
    "    * Here the initial value is the log(odds)\n",
    "$$ F_{0}(x) = argmin \\sum_{i=1}^n L(y_{i}, \\gamma) $$\n",
    "\n",
    "- Step 2 for m = 1 to M:\n",
    "    * (A) Compute residual r_im (y-p)\n",
    "    * (B) Fit a regression tree to the r_im values and create terminal regions R_jm, for j=1,..,Jm\n",
    "    * (C) For j = 1...Jm compute output of each leaf:\n",
    "    $$\\gamma_{jm} = argmin \\sum_{x_{i} \\in R_{ij}}L(y_{i}, F_{m-1}(x_{i}) + \\gamma)$$\n",
    "Mathmatically (second order Taylor polynomial approximation) we can simplify it to:\n",
    "<font color=blue>\n",
    "$$\\gamma = \\frac{\\sum residual}{p(1-p)}$$\n",
    "    </font>\n",
    "    * (D) Update log(odds) of each obs\n",
    "$$F_{m}(x)=F_{m-1}(x)+\\nu*\\sum_{j=1}^{J_{m}}\\gamma_{im}I (x\\in R_{jm})$$\n",
    "For classification, this means to calculate the log(odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "### Key Points\n",
    "- Gradient Boost\n",
    "- Regularization\n",
    "- A Unique Regression Tree\n",
    "- Approximate Greedy Algorithm\n",
    "- Parallel Learning\n",
    "- Weighted Quantile Sketch\n",
    "- Sparsity-Aware Split Finding\n",
    "- Cache-Aware Access\n",
    "- Blocks for Out-of-Core Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
