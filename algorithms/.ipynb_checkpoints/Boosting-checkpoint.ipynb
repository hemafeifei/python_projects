{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatQuest Notes\n",
    "> last updated by Wei, Apr 2020\n",
    "\n",
    "## Part 1 Gradient Bossting\n",
    "### 1.1 Gradient Boost regression\n",
    "#### main ideas\n",
    "- Gradienboost started with an initial value (a single leaf, always can be a mean value), while adaboost started with a weak classifier\n",
    "- a new Tree was built to reduce or represent the __Pseudo Residual__ of previous trees\n",
    "  * Intial leaf -> calculate Residual\n",
    "  * adding First Tree -> calculate leaf output by minimize the leaf loss\n",
    "  * update by calculating Residual\n",
    "- number of leafs can usually be 8 to 32\n",
    "- using learning rate (e.g 0.1) to scale the contribution of a new tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### algorithm\n",
    "- Input data and differential Loss function\n",
    "$$\\{(x_{i}, y_{i})\\}_{i=1} ^ n and L(y_{i}, F(x))$$\n",
    "where mean square error will be used:\n",
    "$$L = \\frac{1}{2}\\sum(y_{i} - p_{i})^2$$\n",
    "- Step 1: Initiallize model with a constant value:\n",
    "    * Here, the initial value is mean\n",
    "$$ F_{0}(x) = argmin \\sum_{i=1}^n L(y_{i}, \\gamma) $$\n",
    "\n",
    "- Step 2: for m = 1 to M:\n",
    "    * (A) Compute residual `r_im`\n",
    "    $$r_{im} = -[\\frac{\\partial L(y_{i}, F(x_{i}))}{\\partial F(x_{i})}]_{F_{x}=F_{m-1}(x)}  for  i=1,...,n $$\n",
    "    * (B) Fit a regression tree to the `r_im` values and create terminal regions `R_jm`, for j=1,..,Jm\n",
    "    * (C) For j = 1...Jm, compute output of each leaf:\n",
    "    $$\\gamma_{jm} = argmin \\sum_{x_{i} \\in R_{ij}}L(y_{i}, F_{m-1}(x_{i}) + \\gamma)$$\n",
    "Mathmatically, it is the average residual:\n",
    "<font color=blue>\n",
    "$$\\gamma = \\frac{\\sum residual}{n obs}$$\n",
    "    </font>\n",
    "    * (D) Update each obs\n",
    "    $$F_{m}(x)=F_{m-1}(x)+\\nu*\\sum_{j=1}^{J_{m}}\\gamma_{im}I (x\\in R_{jm})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Gradient Boosting classification\n",
    "#### main idea\n",
    "- started with an initial value __log(odds)__ (e.g log (4 to 2) = 0.7 and transform to p = 2/3\n",
    "- the calculation of residual on leaf node is different from regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### algorithm\n",
    "- Input data and differential Loss function, loss function is negative log likelihood:\n",
    "<font color=blue>\n",
    "$$ -\\sum_{i=1}^Ny_{i}*log(p) + (1-y_{i}) * log(1-p)$$\n",
    "    </font>\n",
    "it can be transformed to:\n",
    "$$ -y_{i}*[log(p) - log(1-p)] - log(1-p)$$\n",
    "next:\n",
    "$$-y_{i}*log\\frac{p}{1-p} - log(1-p)$$\n",
    "since we can prove:\n",
    "$$log\\frac{p}{1-p} = log(odds)$$\n",
    "$$log(1-p) = -log(1+e^{log(odds)}))$$\n",
    "we can finally get our loss function to:\n",
    "$$-y_{i}*log(odds) + log(1+e^{log(odds)})$$\n",
    "\n",
    "Also, we can prove that:\n",
    "$$\\frac{\\partial L}{\\partial log(odds)} = - y_{i} + \\frac{e^{log(odds)}}{1+e^{log(odds)}} = -y_{i} + p$$\n",
    "- Step 1: Initiallize model with a constant value:\n",
    "    * Here the initial value is the log(odds)\n",
    "$$ F_{0}(x) = argmin \\sum_{i=1}^n L(y_{i}, \\gamma) $$\n",
    "\n",
    "- Step 2 for m = 1 to M:\n",
    "    * (A) Compute residual r_im (y-p)\n",
    "    * (B) Fit a regression tree to the r_im values and create terminal regions R_jm, for j=1,..,Jm\n",
    "    * (C) For j = 1...Jm compute output of each leaf:\n",
    "    $$\\gamma_{jm} = argmin \\sum_{x_{i} \\in R_{ij}}L(y_{i}, F_{m-1}(x_{i}) + \\gamma)$$\n",
    "Mathmatically (second order Taylor polynomial approximation) we can simplify it to:\n",
    "<font color=blue>\n",
    "$$\\gamma = \\frac{\\sum residual}{p(1-p)}$$\n",
    "    </font>\n",
    "    * (D) Update log(odds) of each obs\n",
    "$$F_{m}(x)=F_{m-1}(x)+\\nu*\\sum_{j=1}^{J_{m}}\\gamma_{im}I (x\\in R_{jm})$$\n",
    "For classification, this means to calculate the log(odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Extreme Gradient Boosting (XGBoost)\n",
    "\n",
    "### Key Points\n",
    "- Gradient Boost\n",
    "- Regularization\n",
    "- A Unique Regression Tree\n",
    "- Approximate Greedy Algorithm\n",
    "- Parallel Learning\n",
    "- Weighted Quantile Sketch\n",
    "- Sparsity-Aware Split Finding\n",
    "- Cache-Aware Access\n",
    "- Blocks for Out-of-Core Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extreme Gradient Boosting Tree\n",
    "#### Gain - Similarity scores of a leaf node\n",
    "$s = \\frac{(\\sum Residual_{i})^2}{\\sum[p_{i} * (1 - p{i})] + \\lambda}$ where $p_{i}$ = pervious\\_porobability\n",
    "- cover was used to control growing tree, default=1 (min_child_wight)\n",
    "    * Regression: number of residuals\n",
    "    * Classification: $\\sum p_{i}*(1-p_{i})$\n",
    "\n",
    "- output for leaf\n",
    "    * calculate $\\frac{\\sum Residual_{i}}{\\sum[p_{i} * (1 - p{i})] + \\lambda}$ for each leaf\n",
    "\n",
    "#### Loss Function\n",
    "$$L = [\\sum_{i=1}^n L(y_{i}, p_{i})] + \\gamma T + \\frac{1}{2}\\lambda O_{value}^2$$\n",
    "- Since $p_{i} = p_{0} + O_{value}$, loss function can be transformed to\n",
    "$$ L(y, p_{i}+O_{value}) \\approx L(y,p_{i}) + [\\frac{d}{dp_{i}}L(y,p_{i})]O_{value} + \\frac{1}{2}[\\frac{d^2}{dp_{i}^2}L(y,p_{i})]O_{value}^2 $$\n",
    "- since g is the gradient or first dirivative, h is the second derivative function:\n",
    "$$L(y, p_{i}+O_{value}) \\approx L(y,p_{i}) + gO_{value} +\\frac{1}{2}hO_{value}^2 $$\n",
    "- mathmatically we can prove that minimal loss means when:\n",
    "$O_{value} = \\frac{-(g_{1}+...+g_{n})}{(h_{1}+...+h_{n}+\\lambda)}$ \n",
    "- also $g_{i} = -(y_{i}-p_{i})$ \n",
    "- and $h_{i}=p_{i}*(1-p_{i})$ for classification or 1 for regression\n",
    "\n",
    "#### Summary\n",
    "XGBoost builds trees by finding the $O_{value}$ that minimizes the loss function\n",
    "\n",
    "#### Optimization\n",
    "- approximate greedy algorithm with quantiles (33 in default)\n",
    "- parallel learning and weighted quantile stetch (need more study)\n",
    "The weight for each observation is the 2nd derivative of the loss function, i.e **Hessian**\n",
    "    * that means weight = 1 for all regression \n",
    "    * and p(1-p) for classification\n",
    "\n",
    "- spasity - missing values\n",
    "    * Gain left and Gain right, and choose the best gain with missing values\n",
    "\n",
    "- cache-aware access\n",
    "    * put gradients and hessians in cache memory and make the calculation fast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further docs\n",
    "-[LightGBM and XGBoost Explained](https://mlexplained.com/2018/01/05/lightgbm-and-xgboost-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
