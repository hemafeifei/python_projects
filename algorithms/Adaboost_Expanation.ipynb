{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "-[1 Intro](#intro)\n",
    "<br>\n",
    "-[2 Algorithm](#algorithm)\n",
    "<br>\n",
    "-[3 Adaboost as a additive model](#additive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "<br>\n",
    "## 1 Intro\n",
    "Adaboost与GBM, XGBoost是工业界十分强大的集成学习算法。\n",
    "<br>\n",
    "关于XGBoost可以查看__[pdf](https://brage.bibsys.no/xmlui/handle/11250/2433761)__。\n",
    "<br>\n",
    "Adaboost is short for \"Adaptive Boosting\", first introduceb by Freund and Schapire in 1996.\n",
    "<br>\n",
    "Adaboost旨在将一组弱分类器转化为一个强分类器，其一般形式为：\n",
    "<br>\n",
    "        $$F(x) = sign(\\sum_{m=1}^{M}\\theta_{m}f_{m}(x))$$\n",
    "<br>\n",
    "其中$f_{m}$表示第m个分类器，$\\theta_{m}$表示该分类器相应的权重，因此它是一个弱分类器的组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='algorithm'></a>\n",
    "## 2 Algorithm\n",
    "对于任意给定的数据集，其中：\n",
    "<br>\n",
    "$$x_{i}\\in \\mathbb{R}^d, y_{i} \\in \\{-1, 1\\}.$$\n",
    "<br>\n",
    "初始化每个数据点的权重:\n",
    "$$w(x_{i}, y_{i}) = \\frac{1}{n}, i = 1,...,n.$$\n",
    "<br>\n",
    "**For iteration in range(1,M):**\n",
    "<br>\n",
    "(1)训练弱分类器，找到分类误差最大的那些数据点:\n",
    "<br>\n",
    "$$\\epsilon_{m} = E_{w_{m}}[1_{y\\neq f(x)}]$$\n",
    "<br>\n",
    "(2)计算第m个弱分类器的权重$\\theta_{m}$(有的文章用$\\alpha$):\n",
    "<br>\n",
    "$$\\theta_{m} = \\frac{1}{2}ln(\\frac{1-\\epsilon_{m}}{\\epsilon_{m}})$$\n",
    "<br>\n",
    "注:对于任何弱分类器，其准确率低于50%，权重函数将大于0，被放大。若其准确率高于50%，权重函数将为负数，我们只需要将其分类结果反响，即可得到一个准确率低于50%的分类器。\n",
    "<br>\n",
    "(3)为所有数据更新权重:\n",
    "<br>\n",
    "$$w_{m+1}(x_{i}, y_{i}) = \\frac{w_{m}(x_{i}, y_{i})exp[-\\theta_{m}y_{i}f_{m}(x_{i})]}{Z_{m}}$$\n",
    "<br>\n",
    "以上公式中$Z_{m}$为一个标准化因子(normalization factor)，为了保证更新后所有数据的权重之和仍然为1.\n",
    "<br>\n",
    "以上公式中，对于错误分类的数据点,$y_{i}*f$的值恒为-1， 因此在exp函数的作用下，$w_{m+1}$将被放大。\n",
    "<br>\n",
    "经过M次迭代，我们可以得到最终的加权分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additive'></a>\n",
    "## 3 Adaboost as a additive model\n",
    "关于Adaboost与additive logitic regression的关系可以查看这篇论文:[ Additive logistic regression: a statistical view of boosting](https://web.stanford.edu/~hastie/Papers/AdditiveLogisticRegression/alr.pdf).\n",
    "<br>\n",
    "2000年, Friedman et.al提出了解释adaboost算法的观点，他们指出，adboost实际上试图最小化指数损失函数：\n",
    "<br>\n",
    "$$L(y, F(x)) = \\sum(e^{-y*F(x)})$$\n",
    "<br>\n",
    "损失函数最小即：\n",
    "<br>\n",
    "$$\\frac{\\partial E[e^{-y*F(x)}]}{\\partial F(x)}=0$$\n",
    "<br>\n",
    "由于在Adaboost分类中，y的值只能为-1 or 1, 因此损失函数可以写成：\n",
    "<br>\n",
    "$$E[e^{-y*F(x)}] = e^{F(x)}*P(y=-1\\mid x) + e^{-F(x)}*P(y=1\\mid x)$$\n",
    "<br>\n",
    "继续求微，我们得到：\n",
    "<br>\n",
    "$$\\frac{\\partial E[e^{-y*F(x)}]}{\\partial F(x)}= e^{F(x)}P(y=-1\\mid x) - e^{-F(x)}*P(y=1\\mid x) = 0$$\n",
    "<br>\n",
    "求解F(x),得到\n",
    "$$F(x) = \\frac{1}{2}log\\frac{P(y=1\\mid x)}{P(y=-1\\mid x)}$$\n",
    "我们可以进一步导出符合常规logistic模型的F(x)：\n",
    "$$P(y=1\\mid x) = \\frac{e^{2F(x)}}{1 + e^{2F(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "-[blog](https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
